{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/11/cba4be5a737c6431323b89b5ade818b3bbe1df6e8261c6c70221a767c5d9/xgboost-1.0.2-py3-none-win_amd64.whl (24.6MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "#!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "# from better_profanity import profanity\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge #import ridge \n",
    "\n",
    "#imports\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pop rock folk\n",
    "#balancing response for multilabel data creates information loss.  here is how we dealt with this."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#proportion check\n",
    "df.iloc[:,5:].sum()/df.iloc[:,5:].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import rock balanced data\n",
    "\n",
    "X_train_balance = pd.read_pickle('veclyrics_balanced_rock_doc2vec_train.pkl')\n",
    "X_test_balance = pd.read_pickle('veclyrics_balanced_rock_doc2vec_test.pkl')\n",
    "\n",
    "y_train_balance = pd.read_pickle('response_balanced_rock_doc2vec_train.pkl')\n",
    "y_train_balance = np.array(y_train_balance.iloc[:,3:10])\n",
    "\n",
    "y_test_balance = pd.read_pickle('response_balanced_rock_doc2vec_test.pkl')\n",
    "y_test_balance = np.array(y_test_balance.iloc[:,3:10])\n",
    "\n",
    "y_train_bal1 = y_train_balance[:,0]\n",
    "y_test_bal1 = y_test_balance[:,0]\n",
    "y_train_bal2 = y_train_balance[:,1]\n",
    "y_test_bal2 = y_test_balance[:,1]\n",
    "y_train_bal3 = y_train_balance[:,2]\n",
    "y_test_bal3 = y_test_balance[:,2]\n",
    "y_train_bal4 = y_train_balance[:,3]\n",
    "y_test_bal4 = y_test_balance[:,3]\n",
    "y_train_bal5 = y_train_balance[:,4]\n",
    "y_test_bal5 = y_test_balance[:,4]\n",
    "y_train_bal6 = y_train_balance[:,5]\n",
    "y_test_bal6 = y_test_balance[:,5]\n",
    "y_train_bal7 = y_train_balance[:,6]\n",
    "y_test_bal7 = y_test_balance[:,6]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#import data\n",
    "\n",
    "#doc2vec\n",
    "X_train_vec = pd.read_pickle('veclyrics_doc2vec_train.pkl')\n",
    "X_test_vec = pd.read_pickle('veclyrics_doc2vec_test.pkl')\n",
    "y_train_vec = pd.read_pickle('response_doc2vec_train.pkl')\n",
    "y_test_vec = pd.read_pickle('response_doc2vec_test.pkl')\n",
    "y_train_vec = np.array(y_train_vec.iloc[:,3:10])\n",
    "y_test_vec = np.array(y_test_vec.iloc[:,3:10])\n",
    "\n",
    "y_train_vec1 = y_train_all_vec[:,0]\n",
    "y_test_vec1 = y_test_all_vec[:,0]\n",
    "y_train_vec2 = y_train_all_vec[:,1]\n",
    "y_test_vec2 = y_test_all_vec[:,1]\n",
    "y_train_vec3 = y_train_all_vec[:,2]\n",
    "y_test_vec3 = y_test_all_vec[:,2]\n",
    "y_train_vec4 = y_train_all_vec[:,3]\n",
    "y_test_vec4 = y_test_all_vec[:,3]\n",
    "y_train_vec5 = y_train_all_vec[:,4]\n",
    "y_test_vec5 = y_test_all_vec[:,4]\n",
    "y_train_vec6 = y_train_all_vec[:,5]\n",
    "y_test_vec6 = y_test_all_vec[:,5]\n",
    "y_train_vec7 = y_train_all_vec[:,6]\n",
    "y_test_vec7 = y_test_all_vec[:,6]\n",
    "\n",
    "#pre-split tfidf\n",
    "X_train_tf = pd.read_pickle('veclyrics_tfidf_train.pkl')\n",
    "X_test_tf = pd.read_pickle('veclyrics_tfidf_test.pkl')\n",
    "y_train_all_tf = pd.read_pickle('response_tfidf_train.pkl')\n",
    "y_test_all_tf = pd.read_pickle('response_tfidf_test.pkl')\n",
    "y_train_one_tf = y_train_all_tf.iloc[:,0]\n",
    "y_test_one_tf = y_test_all_tf.iloc[:,0]\n",
    "y_train_all_tf = np.array(y_train_all_tf.iloc[:,3:10])\n",
    "y_test_all_tf = np.array(y_test_all_tf.iloc[:,3:10])\n",
    "\n",
    "#original tfidf\n",
    "veclyrics = pd.read_pickle('veclyrics.pkl')\n",
    "response = np.array(pd.read_pickle('response.pkl'))[:,3:10]\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(veclyrics, response, test_size=0.2, random_state=0)\n",
    "y_train_tf = y_train_tf.astype(int)\n",
    "\n",
    "orgresponse = pd.read_pickle('response.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#pre-split tfidf won't work\n",
    "print(X_test_tf.shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_balance(pred):\n",
    "    \n",
    "    #pred (function input) is array of predictions made using X test\n",
    "    #y_test_vec (imported elsewhere) is array of y test\n",
    "    \n",
    "    acc = []\n",
    "\n",
    "    for i in range(0,7):\n",
    "        check = np.array(pred[:,i]) == y_test_balance[:,i]\n",
    "        accuracy = check.sum()/len(check)\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "    null = pd.DataFrame(pd.DataFrame(y_test_balance).sum()/len(pd.DataFrame(y_test_balance)))\n",
    "    null.iloc[1:,0] = 1-null.iloc[1:,0]\n",
    "    \n",
    "    comp = pd.DataFrame({'accuracy':list(acc), 'null':list(null[0])})\n",
    "    comp['diff'] = comp['accuracy'] - comp['null']\n",
    "    \n",
    "    return(comp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def eval(pred):\n",
    "    \n",
    "    #pred (function input) is array of predictions made using X test\n",
    "    #y_test_vec (imported elsewhere) is array of y test\n",
    "    \n",
    "    acc = []\n",
    "\n",
    "    for i in range(0,7):\n",
    "        check = np.array(pred[:,i]) == y_test_vec[:,i]\n",
    "        accuracy = check.sum()/len(check)\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "    null = pd.DataFrame(pd.DataFrame(y_test_vec).sum()/len(pd.DataFrame(y_test_vec)))\n",
    "    null.iloc[1:,0] = 1-null.iloc[1:,0]\n",
    "    \n",
    "    comp = pd.DataFrame({'accuracy':list(acc), 'null':list(null[0])})\n",
    "    comp['diff'] = comp['accuracy'] - comp['null']\n",
    "    \n",
    "    return(comp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#roy's evaluation code\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def eval_bi_metrics(model,xtest,ypred,ytrue):\n",
    "    \n",
    "     \"\"\"\n",
    "     INPUT:\n",
    "    :param model: model to evaluate\n",
    "    :param xtest: Test model input\n",
    "    :param ypred:predictions\n",
    "    :param ytrue:ground truth\n",
    "     \"\"\"\n",
    "     \"\"\"\n",
    "     OUTPUT:\n",
    "     Classification Report\n",
    "     MCC\n",
    "     Cofusion Matrix\n",
    "     ROC Curve\n",
    "     \"\"\"\n",
    "\n",
    "        \n",
    "    y_pred_proba=model.predict_proba(X_test)    \n",
    "\n",
    "    print (classification_report(ytrue, ypred))\n",
    "\n",
    "    print (\"The Matthews Correlation Coefficient is:\", matthews_corrcoef(ytrue, ypred))\n",
    "#     The MCC is in essence a correlation coefficient value between -1 and +1. \n",
    "#     A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction. \n",
    "\n",
    "    cnf_matrix = metrics.confusion_matrix(ytrue, ypred)\n",
    "    \n",
    "    class_names=[0,1] # name  of classes\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    # create heatmap\n",
    "    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    y_pred_proba = knn.predict_proba(xtest)[::,1]\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize and fit models\n",
    "#doc2vec rock balanced\n",
    "# ~5 min runtime\n",
    "\n",
    "gb_bal1 = GradientBoostingClassifier()\n",
    "gb_bal1.fit(X_train_balance,y_train_bal1)\n",
    "\n",
    "gb_bal2 = GradientBoostingClassifier()\n",
    "gb_bal2.fit(X_train_balance,y_train_bal2)\n",
    "\n",
    "gb_bal3 = GradientBoostingClassifier()\n",
    "gb_bal3.fit(X_train_balance,y_train_bal3)\n",
    "\n",
    "gb_bal4 = GradientBoostingClassifier()\n",
    "gb_bal4.fit(X_train_balance,y_train_bal4)\n",
    "\n",
    "gb_bal5 = GradientBoostingClassifier()\n",
    "gb_bal5.fit(X_train_balance,y_train_bal5)\n",
    "\n",
    "gb_bal6 = GradientBoostingClassifier()\n",
    "gb_bal6.fit(X_train_balance,y_train_bal6)\n",
    "\n",
    "gb_bal7 = GradientBoostingClassifier()\n",
    "gb_bal7.fit(X_train_balance,y_train_bal7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.569961</td>\n",
       "      <td>0.501934</td>\n",
       "      <td>0.068027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.868347</td>\n",
       "      <td>0.868747</td>\n",
       "      <td>-0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.599040</td>\n",
       "      <td>0.574763</td>\n",
       "      <td>0.024276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887822</td>\n",
       "      <td>0.888089</td>\n",
       "      <td>-0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.848473</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>-0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.853808</td>\n",
       "      <td>0.853808</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.923836</td>\n",
       "      <td>0.903962</td>\n",
       "      <td>0.019875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.569961  0.501934  0.068027\n",
       "1  0.868347  0.868747 -0.000400\n",
       "2  0.599040  0.574763  0.024276\n",
       "3  0.887822  0.888089 -0.000267\n",
       "4  0.848473  0.848739 -0.000267\n",
       "5  0.853808  0.853808  0.000000\n",
       "6  0.923836  0.903962  0.019875"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and evaluate models\n",
    "gb_bal1pred = gb_bal1.predict(X_test_balance)\n",
    "gb_bal2pred = gb_bal2.predict(X_test_balance)\n",
    "gb_bal3pred = gb_bal3.predict(X_test_balance)\n",
    "gb_bal4pred = gb_bal4.predict(X_test_balance)\n",
    "gb_bal5pred = gb_bal5.predict(X_test_balance)\n",
    "gb_bal6pred = gb_bal6.predict(X_test_balance)\n",
    "gb_bal7pred = gb_bal7.predict(X_test_balance)\n",
    "\n",
    "gb_pred_bal = pd.DataFrame({'rock':gb_bal1pred, 'singer-songwriter':gb_bal2pred, 'pop':gb_bal3pred, \n",
    "              'metal':gb_bal4pred, 'folk':gb_bal5pred, 'country':gb_bal6pred, 'hip hop / rap':gb_bal7pred})\n",
    "\n",
    "gb_pred_bal = np.array(gb_pred_bal)\n",
    "\n",
    "eval_balance(gb_pred_bal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initialize and fit models\n",
    "#doc2vec imbalanced\n",
    "# ~5 min runtime\n",
    "\n",
    "gb1 = GradientBoostingClassifier()\n",
    "gb1.fit(X_train_vec,y_train_vec1)\n",
    "\n",
    "gb2 = GradientBoostingClassifier()\n",
    "gb2.fit(X_train_vec,y_train_vec2)\n",
    "\n",
    "gb3 = GradientBoostingClassifier()\n",
    "gb3.fit(X_train_vec,y_train_vec3)\n",
    "\n",
    "gb4 = GradientBoostingClassifier()\n",
    "gb4.fit(X_train_vec,y_train_vec4)\n",
    "\n",
    "gb5 = GradientBoostingClassifier()\n",
    "gb5.fit(X_train_vec,y_train_vec5)\n",
    "\n",
    "gb6 = GradientBoostingClassifier()\n",
    "gb6.fit(X_train_vec,y_train_vec6)\n",
    "\n",
    "gb7 = GradientBoostingClassifier()\n",
    "gb7.fit(X_train_vec,y_train_vec7)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#test and evaluate models\n",
    "gb1pred = gb1.predict(X_test_vec)\n",
    "gb2pred = gb2.predict(X_test_vec)\n",
    "gb3pred = gb3.predict(X_test_vec)\n",
    "gb4pred = gb4.predict(X_test_vec)\n",
    "gb5pred = gb5.predict(X_test_vec)\n",
    "gb6pred = gb6.predict(X_test_vec)\n",
    "gb7pred = gb7.predict(X_test_vec)\n",
    "\n",
    "gb_pred_vec = pd.DataFrame({'rock':gb1pred, 'singer-songwriter':gb2pred, 'pop':gb3pred, \n",
    "              'metal':gb4pred, 'folk':gb5pred, 'country':gb6pred, 'hip hop / rap':gb7pred})\n",
    "\n",
    "gb_pred_vec = np.array(gb_pred_vec)\n",
    "\n",
    "eval(gb_pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "non = gb_bal1.predict(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = gsgbc.predict(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5699613178604775"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_bal1.score(X_test_balance,y_test_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5716953448045885"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_test_balance,y_test_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7371264674493063"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_train_balance,y_train_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.05, loss='deviance', max_depth=5,\n",
       "                           max_features='sqrt', max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=3, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=3, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 27 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  9.2min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 31.3min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 40.2min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 47.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 52.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 57.2min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 62.5min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 70.5min\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed: 76.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "             estimator=GradientBoostingClassifier(criterion='friedman_mse',\n",
       "                                                  init=None, learning_rate=0.1,\n",
       "                                                  loss='deviance', max_depth=3,\n",
       "                                                  max_features=None,\n",
       "                                                  max_leaf_nodes=None,\n",
       "                                                  min_impurity_decrease=0.0,\n",
       "                                                  min_impurity_split=None,\n",
       "                                                  min_samples_leaf=1,\n",
       "                                                  min_samples_split=2,\n",
       "                                                  min_weight_fraction_leaf=0.0,\n",
       "                                                  n_estimators=1000,\n",
       "                                                  n_iter_no_change=None,\n",
       "                                                  presort='auto',\n",
       "                                                  random_state=3, subsample=1.0,\n",
       "                                                  tol=0.0001,\n",
       "                                                  validation_fraction=0.1,\n",
       "                                                  verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.01, 0.05, 0.1],\n",
       "                         'max_depth': [4, 5, 6], 'max_features': ['sqrt'],\n",
       "                         'min_samples_leaf': [2, 3, 4]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search Gradient boosting classifier for balanced rock column only\n",
    "gb = GradientBoostingClassifier(n_estimators=1000, random_state=3)\n",
    "# params_gbc = {\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.10, 0.15],\n",
    "#     \"max_depth\": [3, 4, 5, 6],\n",
    "#     \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#     \"max_features\":['sqrt']\n",
    "# }\n",
    "\n",
    "params_gbc = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.10],\n",
    "    \"max_depth\": [4, 5, 6],\n",
    "    \"min_samples_leaf\": [2, 3, 4],\n",
    "    \"max_features\":['sqrt']\n",
    "}\n",
    "\n",
    "#reducing from 10 with kfold to cv = 5 to halve fits\n",
    "gbtune = GridSearchCV(gb, param_grid=params_gbc, cv=10, scoring=\"accuracy\", n_jobs= -1,verbose = 10)\n",
    "\n",
    "gbtune.fit(X_train_balance,y_train_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6567169156883671\n",
      "0.578764839269041\n",
      "0.7879869263607258\n"
     ]
    }
   ],
   "source": [
    "# Best score\n",
    "print(gbtune.best_score_)\n",
    "\n",
    "#test score\n",
    "print(gbtune.score(X_test_balance,y_test_bal1))\n",
    "\n",
    "#train score\n",
    "print(gbtune.score(X_train_balance,y_train_bal1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename = 'gbtune.sav'\n",
    "pickle.dump(gbtune, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "filename = 'gbtune.sav'\n",
    "best = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578764839269041"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.score(X_test_balance,y_test_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  81 | elapsed:  3.1min remaining:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6500133404482391"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search Gradient boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=3)\n",
    "# params_gbc = {\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.10, 0.15],\n",
    "#     \"max_depth\": [3, 4, 5, 6],\n",
    "#     \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#     \"max_features\":['sqrt']\n",
    "# }\n",
    "\n",
    "params_gbc = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.10],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_features\":['sqrt']\n",
    "}\n",
    "\n",
    "#reducing from 10 with kfold to cv = 5 to halve fits\n",
    "gsgbc = GridSearchCV(gb, param_grid=params_gbc, cv=3, scoring=\"accuracy\", n_jobs= -1,verbose = 10)\n",
    "\n",
    "#gb_best = gsgbc.best_estimator_\n",
    "\n",
    "gsgbc.fit(X_train_balance,y_train_bal1)\n",
    "\n",
    "# Best score\n",
    "gsgbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.05, loss='deviance', max_depth=5,\n",
       "                           max_features='sqrt', max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=3, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=3, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6500133404482391"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5716953448045885"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_test_balance,y_test_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, ..., False,  True,  True])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "com = y_test_bal1 == rockpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5716953448045885"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rockparam = gsgbc.best_estimator_\n",
    "rockpred = rockparam.predict(X_test_balance)\n",
    "rockparam.score(X_test_balance,y_test_bal1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.21      2459\n",
      "           1       0.78      0.97      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.69      0.55      0.54     10225\n",
      "weighted avg       0.74      0.77      0.71     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52     10190\n",
      "           1       0.82      0.99      0.90     30708\n",
      "\n",
      "    accuracy                           0.83     40898\n",
      "   macro avg       0.87      0.67      0.71     40898\n",
      "weighted avg       0.85      0.83      0.80     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rockparam.predict(X_test)\n",
    "y_pred_train_rf = rockparam.predict(X_train)\n",
    "print('test report',classification_report(y_test_one, y_pred_rf))\n",
    "print('train report',classification_report(y_train_one, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([11.3081518 , 13.08590802, 13.12594668, 18.31042401, 19.43094103,\n",
       "        20.38704999, 27.49654754, 27.80691083, 28.40115023, 15.38440625,\n",
       "        13.60730775, 11.39607056, 17.16316581, 19.0936594 , 19.81371172,\n",
       "        26.59359026, 28.10804582, 28.44664598, 16.39201482, 12.78171674,\n",
       "        10.58074252, 15.82891202, 17.96121391, 19.3528076 , 27.18937818,\n",
       "        25.85446143, 19.18196519]),\n",
       " 'std_fit_time': array([1.26626   , 0.27484841, 0.23021418, 0.63289025, 0.45235387,\n",
       "        0.38011948, 0.87657483, 0.86250159, 1.3267893 , 0.42033844,\n",
       "        0.75131222, 0.21753072, 0.23068524, 0.82817441, 0.25773188,\n",
       "        0.39976925, 1.24255933, 2.07529208, 0.48978439, 0.97744221,\n",
       "        0.21217617, 0.91924774, 0.60534562, 0.33500888, 0.83075881,\n",
       "        1.36594184, 3.06516744]),\n",
       " 'mean_score_time': array([0.13264505, 0.12400134, 0.12765892, 0.18417533, 0.15658085,\n",
       "        0.16954732, 0.21841685, 0.20811049, 0.1863389 , 0.14062421,\n",
       "        0.12765892, 0.11968017, 0.16023898, 0.16090377, 0.15957395,\n",
       "        0.1855046 , 0.20677956, 0.22373478, 0.13946613, 0.1160237 ,\n",
       "        0.11402957, 0.1522603 , 0.14760574, 0.17320371, 0.20079748,\n",
       "        0.12998517, 0.09607641]),\n",
       " 'std_score_time': array([0.00354908, 0.00448522, 0.00141153, 0.02125574, 0.00880809,\n",
       "        0.02327671, 0.01976301, 0.02644164, 0.00834299, 0.00776848,\n",
       "        0.00635987, 0.01345403, 0.00554215, 0.0061124 , 0.00850226,\n",
       "        0.00423132, 0.00463126, 0.04652488, 0.01916016, 0.00971459,\n",
       "        0.00803388, 0.01011686, 0.00293682, 0.0126512 , 0.01579752,\n",
       "        0.01339093, 0.00542269]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 4, 4, 4, 5, 5, 5, 3, 3, 3, 4, 4, 4, 5, 5, 5,\n",
       "                    3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
       "                    1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3}],\n",
       " 'split0_test_score': array([0.78302648, 0.78302648, 0.78302648, 0.786474  , 0.786474  ,\n",
       "        0.78654735, 0.7893347 , 0.78992151, 0.78977481, 0.79571628,\n",
       "        0.79615639, 0.79608303, 0.79593633, 0.79564293, 0.79666985,\n",
       "        0.79622974, 0.7970366 , 0.7968899 , 0.79454265, 0.79454265,\n",
       "        0.79578963, 0.79659649, 0.79615639, 0.79542287, 0.79527617,\n",
       "        0.79710995, 0.79454265]),\n",
       " 'split1_test_score': array([0.78251302, 0.78251302, 0.78251302, 0.78581383, 0.78588719,\n",
       "        0.78574048, 0.78940805, 0.78955476, 0.78926135, 0.79505611,\n",
       "        0.79556957, 0.79586298, 0.79835693, 0.79784347, 0.79806352,\n",
       "        0.79923715, 0.79967725, 0.79894374, 0.79828358, 0.79762341,\n",
       "        0.7993105 , 0.79710995, 0.79791682, 0.7994572 , 0.79843028,\n",
       "        0.79989731, 0.79799017]),\n",
       " 'split2_test_score': array([0.78249707, 0.78249707, 0.78249707, 0.78543134, 0.78587148,\n",
       "        0.78543134, 0.78851232, 0.78880575, 0.78873239, 0.79401408,\n",
       "        0.79401408, 0.79350059, 0.797902  , 0.79650822, 0.79643486,\n",
       "        0.79768192, 0.79775528, 0.79665493, 0.79504108, 0.79496772,\n",
       "        0.79533451, 0.7963615 , 0.79702171, 0.79768192, 0.79702171,\n",
       "        0.79856221, 0.79746185]),\n",
       " 'mean_test_score': array([0.78267886, 0.78267886, 0.78267886, 0.7859064 , 0.78607756,\n",
       "        0.7859064 , 0.78908504, 0.78942736, 0.7892562 , 0.79492885,\n",
       "        0.79524671, 0.79514891, 0.79739841, 0.79666487, 0.79705609,\n",
       "        0.79771627, 0.79815639, 0.79749621, 0.79595579, 0.79571128,\n",
       "        0.79681158, 0.79668932, 0.79703164, 0.79752066, 0.79690938,\n",
       "        0.79852316, 0.79666487]),\n",
       " 'std_test_score': array([0.0002459 , 0.0002459 , 0.0002459 , 0.00043066, 0.0002804 ,\n",
       "        0.00047047, 0.00040606, 0.00046433, 0.00042558, 0.00070072,\n",
       "        0.00090389, 0.00116895, 0.00105041, 0.00090518, 0.00071881,\n",
       "        0.00122802, 0.00111474, 0.00102806, 0.00165855, 0.0013632 ,\n",
       "        0.00177677, 0.00031252, 0.00071874, 0.00165097, 0.00129012,\n",
       "        0.00113828, 0.00151608]),\n",
       " 'rank_test_score': array([25, 25, 25, 23, 22, 23, 21, 19, 20, 18, 16, 17,  6, 12,  7,  3,  2,\n",
       "         5, 14, 15, 10, 11,  8,  4,  9,  1, 12])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.cv_results_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe=Pipeline([('select',SelectKBest(k=20)), \n",
    "               ('classify', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n",
    "\n",
    "param_test = {'classify__n_estimators':list(range(20,50,5)), \n",
    "              'classify__max_depth':list(range(5,40,5))}\n",
    "gsrf_vec = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=3, verbose = True, n_jobs = -1)\n",
    "gsrf_vec.fit(X_train_vec,y_train_one_vec)\n",
    "print(gsrf_vec.best_params_, gsrf_vec.best_score_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#this is not that good\n",
    "y_pred_gsrf_vec = gsrf_vec.predict(X_test_vec)\n",
    "y_pred_train_gsrf_vec = gsrf_vec.predict(X_train_vec)\n",
    "print('test report',classification_report(y_test_one_vec, y_pred_gsrf_vec))\n",
    "print('train report',classification_report(y_train_one_vec, y_pred_train_gsrf_vec))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_pred_gsrf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.8s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#balance\n",
    "#fitting random forest classifier\n",
    "#works for multilabel response data by default\n",
    "rf_balance = RandomForestClassifier(verbose = True, n_jobs= -1)\n",
    "rf_balance.fit(X_train_balance, y_train_balance)\n",
    "rf_pred_balance = rf_balance.predict(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save random forest model\n",
    "filename = 'rfmulti.sav'\n",
    "pickle.dump(rf_balance, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to load and run default random forest (poor performance as example) and tuned gradient boosting (good performance)\n",
    "\n",
    "#import rock balanced data\n",
    "X_train_balance = pd.read_pickle('veclyrics_balanced_rock_doc2vec_train.pkl')\n",
    "X_test_balance = pd.read_pickle('veclyrics_balanced_rock_doc2vec_test.pkl')\n",
    "\n",
    "y_train_balance = pd.read_pickle('response_balanced_rock_doc2vec_train.pkl')\n",
    "y_train_balance = np.array(y_train_balance.iloc[:,3:10])\n",
    "\n",
    "y_test_balance = pd.read_pickle('response_balanced_rock_doc2vec_test.pkl')\n",
    "y_test_balance = np.array(y_test_balance.iloc[:,3:10])\n",
    "\n",
    "# load tuned gradient boosting single column (balanced rock) model\n",
    "filename = 'gbtune.sav'\n",
    "gbtune = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "# load random forest model for multi-label where only rock is balanced\n",
    "filename = 'rfmulti.sav'\n",
    "rf_balance = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "#predict with rf\n",
    "rf_pred_balance = rf_balance.predict(X_test_balance)\n",
    "\n",
    "#predict with gb\n",
    "gb_pred_balance = gbtune.predict(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "breaX = pd.read_pickle('veclyrics_doc2vec_holdout.pkl')\n",
    "holdoutresponse = gbtune.predict(breaX)\n",
    "pickle.dump(holdoutresponse, open(\"holdoutresponse.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7497"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtune.predict(X_test_balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542750</td>\n",
       "      <td>0.501934</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.866213</td>\n",
       "      <td>0.868747</td>\n",
       "      <td>-0.002534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.579432</td>\n",
       "      <td>0.574763</td>\n",
       "      <td>0.004669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887955</td>\n",
       "      <td>0.888089</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.845271</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>-0.003468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.852074</td>\n",
       "      <td>0.853808</td>\n",
       "      <td>-0.001734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.909564</td>\n",
       "      <td>0.903962</td>\n",
       "      <td>0.005602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.542750  0.501934  0.040816\n",
       "1  0.866213  0.868747 -0.002534\n",
       "2  0.579432  0.574763  0.004669\n",
       "3  0.887955  0.888089 -0.000133\n",
       "4  0.845271  0.848739 -0.003468\n",
       "5  0.852074  0.853808 -0.001734\n",
       "6  0.909564  0.903962  0.005602"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_balance(rf_pred_balance)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#fitting random forest classifier\n",
    "#works for multilabel response data by default\n",
    "rf_vec = RandomForestClassifier(verbose = True, n_jobs= -1)\n",
    "rf_vec.fit(X_train_vec, y_train_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "rf_pred_vec = rf_vec.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "eval(rf_pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize and fit models\n",
    "#rock balanced\n",
    "#few second runtime\n",
    "\n",
    "lr_bal1 = LogisticRegression(solver='liblinear')\n",
    "lr_bal1.fit(X_train_balance,y_train_bal1)\n",
    "\n",
    "lr_bal2 = LogisticRegression(solver='liblinear')\n",
    "lr_bal2.fit(X_train_balance,y_train_bal2)\n",
    "\n",
    "lr_bal3 = LogisticRegression(solver='liblinear')\n",
    "lr_bal3.fit(X_train_balance,y_train_bal3)\n",
    "\n",
    "lr_bal4 = LogisticRegression(solver='liblinear')\n",
    "lr_bal4.fit(X_train_balance,y_train_bal4)\n",
    "\n",
    "lr_bal5 = LogisticRegression(solver='liblinear')\n",
    "lr_bal5.fit(X_train_balance,y_train_bal5)\n",
    "\n",
    "lr_bal6 = LogisticRegression(solver='liblinear')\n",
    "lr_bal6.fit(X_train_balance,y_train_bal6)\n",
    "\n",
    "lr_bal7 = LogisticRegression(solver='liblinear')\n",
    "lr_bal7.fit(X_train_balance,y_train_bal7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.579032</td>\n",
       "      <td>0.501934</td>\n",
       "      <td>0.077098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.865946</td>\n",
       "      <td>0.868747</td>\n",
       "      <td>-0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.604642</td>\n",
       "      <td>0.574763</td>\n",
       "      <td>0.029879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887822</td>\n",
       "      <td>0.888089</td>\n",
       "      <td>-0.000267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.844605</td>\n",
       "      <td>0.848739</td>\n",
       "      <td>-0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.853675</td>\n",
       "      <td>0.853808</td>\n",
       "      <td>-0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.927037</td>\n",
       "      <td>0.903962</td>\n",
       "      <td>0.023076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.579032  0.501934  0.077098\n",
       "1  0.865946  0.868747 -0.002801\n",
       "2  0.604642  0.574763  0.029879\n",
       "3  0.887822  0.888089 -0.000267\n",
       "4  0.844605  0.848739 -0.004135\n",
       "5  0.853675  0.853808 -0.000133\n",
       "6  0.927037  0.903962  0.023076"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and evaluate models\n",
    "lr_bal1pred = lr_bal1.predict(X_test_balance)\n",
    "lr_bal2pred = lr_bal2.predict(X_test_balance)\n",
    "lr_bal3pred = lr_bal3.predict(X_test_balance)\n",
    "lr_bal4pred = lr_bal4.predict(X_test_balance)\n",
    "lr_bal5pred = lr_bal5.predict(X_test_balance)\n",
    "lr_bal6pred = lr_bal6.predict(X_test_balance)\n",
    "lr_bal7pred = lr_bal7.predict(X_test_balance)\n",
    "\n",
    "lr_pred_bal = pd.DataFrame({'rock':lr_bal1pred, 'singer-songwriter':lr_bal2pred, 'pop':lr_bal3pred, \n",
    "              'metal':lr_bal4pred, 'folk':lr_bal5pred, 'country':lr_bal6pred, 'hip hop / rap':lr_bal7pred})\n",
    "\n",
    "lr_pred_bal = np.array(lr_pred_bal)\n",
    "\n",
    "eval_balance(lr_pred_bal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#initialize and fit models\n",
    "#imbalanced\n",
    "#few second runtime\n",
    "\n",
    "lr1 = LogisticRegression(solver='liblinear')\n",
    "lr1.fit(X_train_vec,y_train_vec1)\n",
    "\n",
    "lr2 = LogisticRegression(solver='liblinear')\n",
    "lr2.fit(X_train_vec,y_train_vec2)\n",
    "\n",
    "lr3 = LogisticRegression(solver='liblinear')\n",
    "lr3.fit(X_train_vec,y_train_vec3)\n",
    "\n",
    "lr4 = LogisticRegression(solver='liblinear')\n",
    "lr4.fit(X_train_vec,y_train_vec4)\n",
    "\n",
    "lr5 = LogisticRegression(solver='liblinear')\n",
    "lr5.fit(X_train_vec,y_train_vec5)\n",
    "\n",
    "lr6 = LogisticRegression(solver='liblinear')\n",
    "lr6.fit(X_train_vec,y_train_vec6)\n",
    "\n",
    "lr7 = LogisticRegression(solver='liblinear')\n",
    "lr7.fit(X_train_vec,y_train_vec7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764010</td>\n",
       "      <td>0.759511</td>\n",
       "      <td>0.004499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.805477</td>\n",
       "      <td>0.821711</td>\n",
       "      <td>-0.016235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.570367</td>\n",
       "      <td>0.553545</td>\n",
       "      <td>0.016822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851247</td>\n",
       "      <td>0.850562</td>\n",
       "      <td>0.000685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.771051</td>\n",
       "      <td>0.792665</td>\n",
       "      <td>-0.021614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.807335</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>-0.009584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.933985</td>\n",
       "      <td>0.927335</td>\n",
       "      <td>0.006650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.764010  0.759511  0.004499\n",
       "1  0.805477  0.821711 -0.016235\n",
       "2  0.570367  0.553545  0.016822\n",
       "3  0.851247  0.850562  0.000685\n",
       "4  0.771051  0.792665 -0.021614\n",
       "5  0.807335  0.816919 -0.009584\n",
       "6  0.933985  0.927335  0.006650"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and evaluate models\n",
    "lr1pred = lr1.predict(X_test_vec)\n",
    "lr2pred = lr2.predict(X_test_vec)\n",
    "lr3pred = lr3.predict(X_test_vec)\n",
    "lr4pred = lr4.predict(X_test_vec)\n",
    "lr5pred = lr5.predict(X_test_vec)\n",
    "lr6pred = lr6.predict(X_test_vec)\n",
    "lr7pred = lr7.predict(X_test_vec)\n",
    "\n",
    "lr_pred_vec = pd.DataFrame({'rock':lr1pred, 'singer-songwriter':lr2pred, 'pop':lr3pred, \n",
    "              'metal':lr4pred, 'folk':lr5pred, 'country':lr6pred, 'hip hop / rap':lr7pred})\n",
    "\n",
    "lr_pred_vec = np.array(lr_pred_vec)\n",
    "\n",
    "eval(lr_pred_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
