{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/11/cba4be5a737c6431323b89b5ade818b3bbe1df6e8261c6c70221a767c5d9/xgboost-1.0.2-py3-none-win_amd64.whl (24.6MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "# from better_profanity import profanity\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge #import ridge \n",
    "\n",
    "#imports\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "X_train_vec = pd.read_pickle('veclyrics_doc2vec_train.pkl')\n",
    "X_test_vec = pd.read_pickle('veclyrics_doc2vec_test.pkl')\n",
    "y_train_all_vec = pd.read_pickle('response_doc2vec_train.pkl')\n",
    "y_test_all_vec = pd.read_pickle('response_doc2vec_test.pkl')\n",
    "y_train_all_vec = y_train_all_vec.iloc[:,3:10]\n",
    "y_test_all_vec = y_test_all_vec.iloc[:,3:10]\n",
    "y_train_one_vec = y_train_all_vec.iloc[:,0]\n",
    "y_test_one_vec = y_test_all_vec.iloc[:,0]\n",
    "\n",
    "X_train_tf = pd.read_pickle('veclyrics_tfidf_train.pkl')\n",
    "X_test_tf = pd.read_pickle('veclyrics_tfidf_test.pkl')\n",
    "y_train_all_tf = pd.read_pickle('response_tfidf_train.pkl')\n",
    "y_test_all_tf = pd.read_pickle('response_tfidf_test.pkl')\n",
    "y_train_one_tf = y_train_all_tf.iloc[:,0]\n",
    "y_test_one_tf = y_test_all_tf.iloc[:,0]\n",
    "y_train_all_tf = np.array(y_train_all_tf.iloc[:,3:10])\n",
    "y_test_all_tf = np.array(y_test_all_tf.iloc[:,3:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_veclyrics = pd.read_pickle('veclyrics.pkl')\n",
    "org_response = pd.read_pickle('response.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#veclyrics pkl\n",
    "#response pkl\n",
    "#try tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10225, 22079)\n",
      "(40897, 42176)\n",
      "  (0, 18515)\t0.1460772490090009\n",
      "  (0, 18548)\t0.1460772490090009\n",
      "  (0, 8186)\t0.08440532972158121\n",
      "  (0, 14100)\t0.16516091011374892\n",
      "  (0, 9675)\t0.06762650934048678\n",
      "  (0, 2196)\t0.10639982409228783\n",
      "  (0, 6103)\t0.11174050260998751\n",
      "  (0, 8502)\t0.08615880520775451\n",
      "  (0, 2380)\t0.07186377396379419\n",
      "  (0, 6479)\t0.13735673503507304\n",
      "  (0, 3145)\t0.08331333749029673\n",
      "  (0, 18998)\t0.04264589090558779\n",
      "  (0, 21535)\t0.056012672430605134\n",
      "  (0, 16306)\t0.06212225097034479\n",
      "  (0, 4197)\t0.05284633574725753\n",
      "  (0, 19343)\t0.04850050502020195\n",
      "  (0, 7857)\t0.13337761218264665\n",
      "  (0, 6934)\t0.07718920013310418\n",
      "  (0, 494)\t0.11762554776144347\n",
      "  (0, 5709)\t0.11540995450149012\n",
      "  (0, 16071)\t0.1581531756562561\n",
      "  (0, 3165)\t0.16136368431967227\n",
      "  (0, 2763)\t0.1757999166522441\n",
      "  (0, 16037)\t0.0854668119601226\n",
      "  (0, 15125)\t0.15179296736764486\n",
      "  :\t:\n",
      "  (10224, 13359)\t0.09994741852330138\n",
      "  (10224, 19476)\t0.19995112297970877\n",
      "  (10224, 16583)\t0.05654182171055604\n",
      "  (10224, 10479)\t0.07313530439474636\n",
      "  (10224, 16019)\t0.11273901359038375\n",
      "  (10224, 19341)\t0.03717822702664911\n",
      "  (10224, 9042)\t0.04493084215284125\n",
      "  (10224, 11183)\t0.08342306786837111\n",
      "  (10224, 9008)\t0.17100069460969844\n",
      "  (10224, 3818)\t0.02868018163956014\n",
      "  (10224, 7993)\t0.04232232786427779\n",
      "  (10224, 19627)\t0.052915056105113066\n",
      "  (10224, 7987)\t0.04055210696092923\n",
      "  (10224, 7738)\t0.07523952335093755\n",
      "  (10224, 21868)\t0.04040626204575972\n",
      "  (10224, 6746)\t0.5748580546895784\n",
      "  (10224, 1614)\t0.1894278552786663\n",
      "  (10224, 21056)\t0.04412305805511499\n",
      "  (10224, 18998)\t0.09158053812264229\n",
      "  (10224, 21535)\t0.04009507578145886\n",
      "  (10224, 11812)\t0.3355400674233011\n",
      "  (10224, 11527)\t0.08811487755206422\n",
      "  (10224, 863)\t0.041031999337220806\n",
      "  (10224, 21800)\t0.15048825413487518\n",
      "  (10224, 10997)\t0.08307836090598052\n"
     ]
    }
   ],
   "source": [
    "print(X_test_tf.shape)\n",
    "print(X_train_tf.shape)\n",
    "print(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1026           18.33s\n",
      "         2           1.0875           17.53s\n",
      "         3           1.0753           16.95s\n",
      "         4           1.0650           16.70s\n",
      "         5           1.0563           16.44s\n",
      "         6           1.0484           16.24s\n",
      "         7           1.0413           16.05s\n",
      "         8           1.0359           15.82s\n",
      "         9           1.0302           15.66s\n",
      "        10           1.0255           15.47s\n",
      "        20           0.9930           14.13s\n",
      "        30           0.9724           12.37s\n",
      "        40           0.9583           10.52s\n",
      "        50           0.9480            8.81s\n",
      "        60           0.9395            7.01s\n",
      "        70           0.9326            5.23s\n",
      "        80           0.9260            3.49s\n",
      "        90           0.9206            1.74s\n",
      "       100           0.9156            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=True,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doesn't take n_jobs?\n",
    "#also doesn't want to take multilabel by default\n",
    "gb_vec = GradientBoostingClassifier(verbose = True)\n",
    "gb_vec.fit(X_train_vec, y_train_one_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1026            2.81m\n",
      "         2           1.0880            2.70m\n",
      "         3           1.0768            2.51m\n",
      "         4           1.0678            2.49m\n",
      "         5           1.0606            2.50m\n",
      "         6           1.0537            2.51m\n",
      "         7           1.0477            2.49m\n",
      "         8           1.0428            2.43m\n",
      "         9           1.0383            2.41m\n",
      "        10           1.0340            2.37m\n",
      "        20           1.0042            2.10m\n",
      "        30           0.9873            1.89m\n",
      "        40           0.9758            1.63m\n",
      "        50           0.9670            1.37m\n",
      "        60           0.9597            1.10m\n",
      "        70           0.9535           48.70s\n",
      "        80           0.9477           31.94s\n",
      "        90           0.9428           15.92s\n",
      "       100           0.9378            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=True,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doesn't take n_jobs?\n",
    "#also doesn't want to take multilabel by default\n",
    "gb_tf = GradientBoostingClassifier(verbose = True)\n",
    "gb_tf.fit(X_train_tf, y_train_one_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7680195599022005"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gb is slow and, for pop at least, way less accurate with default parameters than rf.  \n",
    "#also, it doesn't easily take multilabel.\n",
    "gb_vec.score(X_test_vec,y_test_one_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 42176 and input n_features is 22079 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-58be343b1a65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#gb is slow and, for pop at least, way less accurate with default parameters than rf.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#also, it doesn't easily take multilabel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mgb_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_one_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    355\u001b[0m         \"\"\"\n\u001b[0;32m    356\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2131\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2132\u001b[0m         \"\"\"\n\u001b[1;32m-> 2133\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2134\u001b[0m         \u001b[0mencoded_labels\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2135\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_prediction_to_decision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   2087\u001b[0m         \"\"\"\n\u001b[0;32m   2088\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2089\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2090\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[1;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1671\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1672\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[0;32m   1673\u001b[0m                        raw_predictions)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1655\u001b[0m         \u001b[1;34m\"\"\"Check input and compute raw predictions of the init estimtor.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1658\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    400\u001b[0m                              \u001b[1;34m\"match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                              \u001b[1;34m\"input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 42176 and input n_features is 22079 "
     ]
    }
   ],
   "source": [
    "#gb is slow and, for pop at least, way less accurate with default parameters than rf.  \n",
    "#also, it doesn't easily take multilabel.\n",
    "gb_tf.score(X_test_tf,y_test_one_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10225x22079 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 604490 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  81 | elapsed:  4.1min remaining:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7985231551665118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search Gradient boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=3)\n",
    "# params_gbc = {\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.10, 0.15],\n",
    "#     \"max_depth\": [3, 4, 5, 6],\n",
    "#     \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#     \"max_features\":['sqrt']\n",
    "# }\n",
    "\n",
    "params_gbc = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.10],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_features\":['sqrt']\n",
    "}\n",
    "\n",
    "#reducing from 10 with kfold to cv = 5 to halve fits\n",
    "gsgbc = GridSearchCV(gb, param_grid=params_gbc, cv=3, scoring=\"accuracy\", n_jobs= -1,verbose = 10)\n",
    "\n",
    "#gb_best = gsgbc.best_estimator_\n",
    "\n",
    "gsgbc.fit(X_train,y_train_one)\n",
    "\n",
    "# Best score\n",
    "gsgbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699755501222494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_test,y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockparam = gsgbc.best_estimator_\n",
    "rockpred = rockparam.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.21      2459\n",
      "           1       0.78      0.97      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.69      0.55      0.54     10225\n",
      "weighted avg       0.74      0.77      0.71     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52     10190\n",
      "           1       0.82      0.99      0.90     30708\n",
      "\n",
      "    accuracy                           0.83     40898\n",
      "   macro avg       0.87      0.67      0.71     40898\n",
      "weighted avg       0.85      0.83      0.80     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rockparam.predict(X_test)\n",
    "y_pred_train_rf = rockparam.predict(X_train)\n",
    "print('test report',classification_report(y_test_one, y_pred_rf))\n",
    "print('train report',classification_report(y_train_one, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([11.3081518 , 13.08590802, 13.12594668, 18.31042401, 19.43094103,\n",
       "        20.38704999, 27.49654754, 27.80691083, 28.40115023, 15.38440625,\n",
       "        13.60730775, 11.39607056, 17.16316581, 19.0936594 , 19.81371172,\n",
       "        26.59359026, 28.10804582, 28.44664598, 16.39201482, 12.78171674,\n",
       "        10.58074252, 15.82891202, 17.96121391, 19.3528076 , 27.18937818,\n",
       "        25.85446143, 19.18196519]),\n",
       " 'std_fit_time': array([1.26626   , 0.27484841, 0.23021418, 0.63289025, 0.45235387,\n",
       "        0.38011948, 0.87657483, 0.86250159, 1.3267893 , 0.42033844,\n",
       "        0.75131222, 0.21753072, 0.23068524, 0.82817441, 0.25773188,\n",
       "        0.39976925, 1.24255933, 2.07529208, 0.48978439, 0.97744221,\n",
       "        0.21217617, 0.91924774, 0.60534562, 0.33500888, 0.83075881,\n",
       "        1.36594184, 3.06516744]),\n",
       " 'mean_score_time': array([0.13264505, 0.12400134, 0.12765892, 0.18417533, 0.15658085,\n",
       "        0.16954732, 0.21841685, 0.20811049, 0.1863389 , 0.14062421,\n",
       "        0.12765892, 0.11968017, 0.16023898, 0.16090377, 0.15957395,\n",
       "        0.1855046 , 0.20677956, 0.22373478, 0.13946613, 0.1160237 ,\n",
       "        0.11402957, 0.1522603 , 0.14760574, 0.17320371, 0.20079748,\n",
       "        0.12998517, 0.09607641]),\n",
       " 'std_score_time': array([0.00354908, 0.00448522, 0.00141153, 0.02125574, 0.00880809,\n",
       "        0.02327671, 0.01976301, 0.02644164, 0.00834299, 0.00776848,\n",
       "        0.00635987, 0.01345403, 0.00554215, 0.0061124 , 0.00850226,\n",
       "        0.00423132, 0.00463126, 0.04652488, 0.01916016, 0.00971459,\n",
       "        0.00803388, 0.01011686, 0.00293682, 0.0126512 , 0.01579752,\n",
       "        0.01339093, 0.00542269]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 4, 4, 4, 5, 5, 5, 3, 3, 3, 4, 4, 4, 5, 5, 5,\n",
       "                    3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
       "                    1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3}],\n",
       " 'split0_test_score': array([0.78302648, 0.78302648, 0.78302648, 0.786474  , 0.786474  ,\n",
       "        0.78654735, 0.7893347 , 0.78992151, 0.78977481, 0.79571628,\n",
       "        0.79615639, 0.79608303, 0.79593633, 0.79564293, 0.79666985,\n",
       "        0.79622974, 0.7970366 , 0.7968899 , 0.79454265, 0.79454265,\n",
       "        0.79578963, 0.79659649, 0.79615639, 0.79542287, 0.79527617,\n",
       "        0.79710995, 0.79454265]),\n",
       " 'split1_test_score': array([0.78251302, 0.78251302, 0.78251302, 0.78581383, 0.78588719,\n",
       "        0.78574048, 0.78940805, 0.78955476, 0.78926135, 0.79505611,\n",
       "        0.79556957, 0.79586298, 0.79835693, 0.79784347, 0.79806352,\n",
       "        0.79923715, 0.79967725, 0.79894374, 0.79828358, 0.79762341,\n",
       "        0.7993105 , 0.79710995, 0.79791682, 0.7994572 , 0.79843028,\n",
       "        0.79989731, 0.79799017]),\n",
       " 'split2_test_score': array([0.78249707, 0.78249707, 0.78249707, 0.78543134, 0.78587148,\n",
       "        0.78543134, 0.78851232, 0.78880575, 0.78873239, 0.79401408,\n",
       "        0.79401408, 0.79350059, 0.797902  , 0.79650822, 0.79643486,\n",
       "        0.79768192, 0.79775528, 0.79665493, 0.79504108, 0.79496772,\n",
       "        0.79533451, 0.7963615 , 0.79702171, 0.79768192, 0.79702171,\n",
       "        0.79856221, 0.79746185]),\n",
       " 'mean_test_score': array([0.78267886, 0.78267886, 0.78267886, 0.7859064 , 0.78607756,\n",
       "        0.7859064 , 0.78908504, 0.78942736, 0.7892562 , 0.79492885,\n",
       "        0.79524671, 0.79514891, 0.79739841, 0.79666487, 0.79705609,\n",
       "        0.79771627, 0.79815639, 0.79749621, 0.79595579, 0.79571128,\n",
       "        0.79681158, 0.79668932, 0.79703164, 0.79752066, 0.79690938,\n",
       "        0.79852316, 0.79666487]),\n",
       " 'std_test_score': array([0.0002459 , 0.0002459 , 0.0002459 , 0.00043066, 0.0002804 ,\n",
       "        0.00047047, 0.00040606, 0.00046433, 0.00042558, 0.00070072,\n",
       "        0.00090389, 0.00116895, 0.00105041, 0.00090518, 0.00071881,\n",
       "        0.00122802, 0.00111474, 0.00102806, 0.00165855, 0.0013632 ,\n",
       "        0.00177677, 0.00031252, 0.00071874, 0.00165097, 0.00129012,\n",
       "        0.00113828, 0.00151608]),\n",
       " 'rank_test_score': array([25, 25, 25, 23, 22, 23, 21, 19, 20, 18, 16, 17,  6, 12,  7,  3,  2,\n",
       "         5, 14, 15, 10, 11,  8,  4,  9,  1, 12])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gsrf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-21bcada65dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgsrf_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgsrf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_one_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gsrf' is not defined"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([('select',SelectKBest(k=20)), \n",
    "               ('classify', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n",
    "\n",
    "param_test = {'classify__n_estimators':list(range(20,50,5)), \n",
    "              'classify__max_depth':list(range(5,40,5))}\n",
    "gsrf_vec = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=3, verbose = True, n_jobs = -1)\n",
    "gsrf_vec.fit(X_train_vec,y_train_one_vec)\n",
    "print(gsrf_vec.best_params_, gsrf_vec.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.09      0.16      2459\n",
      "           1       0.77      0.99      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.75      0.54      0.51     10225\n",
      "weighted avg       0.76      0.77      0.70     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.27      0.43     10190\n",
      "           1       0.81      1.00      0.89     30708\n",
      "\n",
      "    accuracy                           0.82     40898\n",
      "   macro avg       0.89      0.64      0.66     40898\n",
      "weighted avg       0.85      0.82      0.78     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test and train reports appear to be somehow flipped?\n",
    "#regardless this is not that good\n",
    "y_pred_gsrf_vec = gsrf_vec.predict(X_test_vec)\n",
    "y_pred_train_gsrf_vec = gsrf_vec.predict(X_train_vec)\n",
    "print('test report',classification_report(y_test_one_vec, y_pred_gsrf_vec))\n",
    "print('train report',classification_report(y_train_one_vec, y_pred_train_gsrf_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gsrf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.9s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "                       oob_score=False, random_state=None, verbose=True,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting random forest classifier\n",
    "#works for multilabel response data by default\n",
    "rf_vec = RandomForestClassifier(verbose = True, n_jobs= -1)\n",
    "rf_vec.fit(X_train_vec, y_train_all_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       null    rf_vec\n",
      "rock               0.759511  0.745526\n",
      "singer-songwriter  0.821711  0.813496\n",
      "pop                0.553545  0.551491\n",
      "metal              0.850562  0.849487\n",
      "folk               0.792665  0.780049\n",
      "country            0.816919  0.811051\n",
      "hip hop / rap      0.927335  0.929976\n"
     ]
    }
   ],
   "source": [
    "comp = pd.DataFrame(y_test_all_vec.sum()/len(y_test_all_vec)).rename(columns = {0:'null'})\n",
    "comp['rf_vec'] = acc\n",
    "comp.iloc[1:,0] = 1-comp.iloc[1:,0]\n",
    "print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7455256723716381, 0.8134963325183374, 0.5514914425427873, 0.8494865525672372, 0.7800488997555012, 0.8110513447432763, 0.9299755501222494]\n",
      "rock                 0.759511\n",
      "singer-songwriter    0.178289\n",
      "pop                  0.446455\n",
      "metal                0.149438\n",
      "folk                 0.207335\n",
      "country              0.183081\n",
      "hip hop / rap        0.072665\n",
      "dtype: float64\n",
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      7766\n",
      "           1       0.20      0.02      0.03      1823\n",
      "           2       0.50      0.27      0.35      4565\n",
      "           3       0.31      0.01      0.01      1528\n",
      "           4       0.25      0.03      0.05      2120\n",
      "           5       0.32      0.03      0.05      1872\n",
      "           6       0.83      0.05      0.09       743\n",
      "\n",
      "   micro avg       0.69      0.43      0.53     20417\n",
      "   macro avg       0.46      0.19      0.21     20417\n",
      "weighted avg       0.53      0.43      0.42     20417\n",
      " samples avg       0.70      0.48      0.52     20417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#y_pred_rf vs y_test\n",
    "y_pred_rf_vec = rf_vec.predict(X_test_vec)\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(0,7):\n",
    "    check = y_pred_rf_vec[:,i] == np.array(y_test_all_vec.iloc[:,i])\n",
    "    accuracy = check.sum()/len(check)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "#prints raw accuracy score by genre\n",
    "print(acc)\n",
    "\n",
    "#average accuracy score\n",
    "print('average accuracy score: ',sum(acc)/len(acc))\n",
    "\n",
    "#proportion of 1s in each genre column\n",
    "print(y_test_all_vec.sum()/len(y_test_all_vec))\n",
    "\n",
    "#why is classification report not matching (anywhere) the manually calculated raw accuracies?\n",
    "print('test report',classification_report(y_test_all_vec, y_pred_rf_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1710513447432763"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appears to only be correct when whole vector of predictions is correct.  our metrics should check genre by genre.\n",
    "rf.score(X_test,y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85      7766\n",
      "           1       0.19      0.02      0.03      1823\n",
      "           2       0.49      0.27      0.35      4565\n",
      "           3       0.35      0.01      0.01      1528\n",
      "           4       0.22      0.03      0.05      2120\n",
      "           5       0.25      0.02      0.04      1872\n",
      "           6       0.65      0.03      0.05       743\n",
      "\n",
      "   micro avg       0.69      0.42      0.53     20417\n",
      "   macro avg       0.42      0.19      0.20     20417\n",
      "weighted avg       0.52      0.42      0.42     20417\n",
      " samples avg       0.70      0.47      0.52     20417\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     30708\n",
      "           1       1.00      0.88      0.93      7281\n",
      "           2       1.00      0.97      0.98     18372\n",
      "           3       1.00      0.87      0.93      6221\n",
      "           4       1.00      0.90      0.94      8429\n",
      "           5       1.00      0.89      0.94      7519\n",
      "           6       1.00      0.90      0.95      3216\n",
      "\n",
      "   micro avg       1.00      0.95      0.97     81746\n",
      "   macro avg       1.00      0.91      0.95     81746\n",
      "weighted avg       1.00      0.95      0.97     81746\n",
      " samples avg       0.99      0.96      0.96     81746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "print('test report',classification_report(y_test_all, y_pred_rf))\n",
    "print('train report',classification_report(y_train_all, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't know n_jobs\n",
    "#won't take multilabel by default\n",
    "#doesn't seem to be working\n",
    "\n",
    "#svc = SVC(probability=True, verbose = True)\n",
    "#svc.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test_one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
