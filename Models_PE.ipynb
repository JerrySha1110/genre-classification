{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/11/cba4be5a737c6431323b89b5ade818b3bbe1df6e8261c6c70221a767c5d9/xgboost-1.0.2-py3-none-win_amd64.whl (24.6MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "# from better_profanity import profanity\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge #import ridge \n",
    "\n",
    "#imports\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "X_train = pd.read_pickle('veclyrics_doc2vec_train.pkl')\n",
    "X_test = pd.read_pickle('veclyrics_doc2vec_test.pkl')\n",
    "y_train_all = pd.read_pickle('response_doc2vec_train.pkl')\n",
    "y_test_all = pd.read_pickle('response_doc2vec_test.pkl')\n",
    "y_train_all = y_train_all.iloc[:,3:10]\n",
    "y_test_all = y_test_all.iloc[:,3:10]\n",
    "y_train_one = y_train_all.iloc[:,0]\n",
    "y_test_one = y_test_all.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1026           17.09s\n",
      "         2           1.0875           17.30s\n",
      "         3           1.0753           17.25s\n",
      "         4           1.0650           17.21s\n",
      "         5           1.0563           16.90s\n",
      "         6           1.0484           16.65s\n",
      "         7           1.0413           16.48s\n",
      "         8           1.0359           16.26s\n",
      "         9           1.0302           16.00s\n",
      "        10           1.0255           15.87s\n",
      "        20           0.9930           14.15s\n",
      "        30           0.9724           12.32s\n",
      "        40           0.9583           10.52s\n",
      "        50           0.9480            8.81s\n",
      "        60           0.9395            6.99s\n",
      "        70           0.9326            5.21s\n",
      "        80           0.9260            3.49s\n",
      "        90           0.9206            1.74s\n",
      "       100           0.9156            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=True,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doesn't take n_jobs?\n",
    "#also doesn't want to take multilabel by default\n",
    "gb = GradientBoostingClassifier(verbose = True)\n",
    "gb.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7680195599022005"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gb is slow and, for pop at least, way less accurate with default parameters than rf.  \n",
    "#also, it doesn't easily take multilabel.\n",
    "gb.score(X_test,y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  81 | elapsed:  4.1min remaining:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7985231551665118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=3)\n",
    "# params_gbc = {\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.10, 0.15],\n",
    "#     \"max_depth\": [3, 4, 5, 6],\n",
    "#     \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#     \"max_features\":['sqrt']\n",
    "# }\n",
    "\n",
    "params_gbc = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.10],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_features\":['sqrt']\n",
    "}\n",
    "\n",
    "#reducing from 10 with kfold to cv = 5 to halve fits\n",
    "gsgbc = GridSearchCV(gb, param_grid=params_gbc, cv=3, scoring=\"accuracy\", n_jobs= -1,verbose = 10)\n",
    "\n",
    "#gb_best = gsgbc.best_estimator_\n",
    "\n",
    "gsgbc.fit(X_train,y_train_one)\n",
    "\n",
    "# Best score\n",
    "gsgbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699755501222494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_test,y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockparam = gsgbc.best_estimator_\n",
    "rockpred = rockparam.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.21      2459\n",
      "           1       0.78      0.97      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.69      0.55      0.54     10225\n",
      "weighted avg       0.74      0.77      0.71     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52     10190\n",
      "           1       0.82      0.99      0.90     30708\n",
      "\n",
      "    accuracy                           0.83     40898\n",
      "   macro avg       0.87      0.67      0.71     40898\n",
      "weighted avg       0.85      0.83      0.80     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rockparam.predict(X_test)\n",
    "y_pred_train_rf = rockparam.predict(X_train)\n",
    "print('test report',classification_report(y_test_one, y_pred_rf))\n",
    "print('train report',classification_report(y_train_one, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([11.3081518 , 13.08590802, 13.12594668, 18.31042401, 19.43094103,\n",
       "        20.38704999, 27.49654754, 27.80691083, 28.40115023, 15.38440625,\n",
       "        13.60730775, 11.39607056, 17.16316581, 19.0936594 , 19.81371172,\n",
       "        26.59359026, 28.10804582, 28.44664598, 16.39201482, 12.78171674,\n",
       "        10.58074252, 15.82891202, 17.96121391, 19.3528076 , 27.18937818,\n",
       "        25.85446143, 19.18196519]),\n",
       " 'std_fit_time': array([1.26626   , 0.27484841, 0.23021418, 0.63289025, 0.45235387,\n",
       "        0.38011948, 0.87657483, 0.86250159, 1.3267893 , 0.42033844,\n",
       "        0.75131222, 0.21753072, 0.23068524, 0.82817441, 0.25773188,\n",
       "        0.39976925, 1.24255933, 2.07529208, 0.48978439, 0.97744221,\n",
       "        0.21217617, 0.91924774, 0.60534562, 0.33500888, 0.83075881,\n",
       "        1.36594184, 3.06516744]),\n",
       " 'mean_score_time': array([0.13264505, 0.12400134, 0.12765892, 0.18417533, 0.15658085,\n",
       "        0.16954732, 0.21841685, 0.20811049, 0.1863389 , 0.14062421,\n",
       "        0.12765892, 0.11968017, 0.16023898, 0.16090377, 0.15957395,\n",
       "        0.1855046 , 0.20677956, 0.22373478, 0.13946613, 0.1160237 ,\n",
       "        0.11402957, 0.1522603 , 0.14760574, 0.17320371, 0.20079748,\n",
       "        0.12998517, 0.09607641]),\n",
       " 'std_score_time': array([0.00354908, 0.00448522, 0.00141153, 0.02125574, 0.00880809,\n",
       "        0.02327671, 0.01976301, 0.02644164, 0.00834299, 0.00776848,\n",
       "        0.00635987, 0.01345403, 0.00554215, 0.0061124 , 0.00850226,\n",
       "        0.00423132, 0.00463126, 0.04652488, 0.01916016, 0.00971459,\n",
       "        0.00803388, 0.01011686, 0.00293682, 0.0126512 , 0.01579752,\n",
       "        0.01339093, 0.00542269]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 4, 4, 4, 5, 5, 5, 3, 3, 3, 4, 4, 4, 5, 5, 5,\n",
       "                    3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
       "                    1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3}],\n",
       " 'split0_test_score': array([0.78302648, 0.78302648, 0.78302648, 0.786474  , 0.786474  ,\n",
       "        0.78654735, 0.7893347 , 0.78992151, 0.78977481, 0.79571628,\n",
       "        0.79615639, 0.79608303, 0.79593633, 0.79564293, 0.79666985,\n",
       "        0.79622974, 0.7970366 , 0.7968899 , 0.79454265, 0.79454265,\n",
       "        0.79578963, 0.79659649, 0.79615639, 0.79542287, 0.79527617,\n",
       "        0.79710995, 0.79454265]),\n",
       " 'split1_test_score': array([0.78251302, 0.78251302, 0.78251302, 0.78581383, 0.78588719,\n",
       "        0.78574048, 0.78940805, 0.78955476, 0.78926135, 0.79505611,\n",
       "        0.79556957, 0.79586298, 0.79835693, 0.79784347, 0.79806352,\n",
       "        0.79923715, 0.79967725, 0.79894374, 0.79828358, 0.79762341,\n",
       "        0.7993105 , 0.79710995, 0.79791682, 0.7994572 , 0.79843028,\n",
       "        0.79989731, 0.79799017]),\n",
       " 'split2_test_score': array([0.78249707, 0.78249707, 0.78249707, 0.78543134, 0.78587148,\n",
       "        0.78543134, 0.78851232, 0.78880575, 0.78873239, 0.79401408,\n",
       "        0.79401408, 0.79350059, 0.797902  , 0.79650822, 0.79643486,\n",
       "        0.79768192, 0.79775528, 0.79665493, 0.79504108, 0.79496772,\n",
       "        0.79533451, 0.7963615 , 0.79702171, 0.79768192, 0.79702171,\n",
       "        0.79856221, 0.79746185]),\n",
       " 'mean_test_score': array([0.78267886, 0.78267886, 0.78267886, 0.7859064 , 0.78607756,\n",
       "        0.7859064 , 0.78908504, 0.78942736, 0.7892562 , 0.79492885,\n",
       "        0.79524671, 0.79514891, 0.79739841, 0.79666487, 0.79705609,\n",
       "        0.79771627, 0.79815639, 0.79749621, 0.79595579, 0.79571128,\n",
       "        0.79681158, 0.79668932, 0.79703164, 0.79752066, 0.79690938,\n",
       "        0.79852316, 0.79666487]),\n",
       " 'std_test_score': array([0.0002459 , 0.0002459 , 0.0002459 , 0.00043066, 0.0002804 ,\n",
       "        0.00047047, 0.00040606, 0.00046433, 0.00042558, 0.00070072,\n",
       "        0.00090389, 0.00116895, 0.00105041, 0.00090518, 0.00071881,\n",
       "        0.00122802, 0.00111474, 0.00102806, 0.00165855, 0.0013632 ,\n",
       "        0.00177677, 0.00031252, 0.00071874, 0.00165097, 0.00129012,\n",
       "        0.00113828, 0.00151608]),\n",
       " 'rank_test_score': array([25, 25, 25, 23, 22, 23, 21, 19, 20, 18, 16, 17,  6, 12,  7,  3,  2,\n",
       "         5, 14, 15, 10, 11,  8,  4,  9,  1, 12])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:   53.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__max_depth': 10, 'classify__n_estimators': 45} 0.6881153938106696\n"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([('select',SelectKBest(k=20)), \n",
    "               ('classify', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n",
    "\n",
    "param_test = {'classify__n_estimators':list(range(20,50,5)), \n",
    "              'classify__max_depth':list(range(5,40,5))}\n",
    "gsearch = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=3, verbose = True, n_jobs = -1)\n",
    "gsearch.fit(X_test,y_test_one)\n",
    "print(gsearch.best_params_, gsearch.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.29      0.45      2459\n",
      "           1       0.82      1.00      0.90      7766\n",
      "\n",
      "    accuracy                           0.83     10225\n",
      "   macro avg       0.91      0.64      0.67     10225\n",
      "weighted avg       0.86      0.83      0.79     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.13      0.23     10190\n",
      "           1       0.77      0.99      0.87     30708\n",
      "\n",
      "    accuracy                           0.77     40898\n",
      "   macro avg       0.77      0.56      0.55     40898\n",
      "weighted avg       0.77      0.77      0.71     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test and train reports appear to be somehow flipped?\n",
    "#regardless this is not that good\n",
    "y_pred_gs = gsearch.predict(X_test)\n",
    "y_pred_train_gs = gsearch.predict(X_train)\n",
    "print('test report',classification_report(y_test_one, y_pred_gs))\n",
    "print('train report',classification_report(y_train_one, y_pred_train_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from sklearn.pipeline import make_pipeline\n",
    "# select = SelectKBest(k = 20)\n",
    "# clf = RandomForestClassifier(random_state = 10, warm_start = True, \n",
    "#                                   n_estimators = 26,\n",
    "#                                   max_depth = 6, \n",
    "#                                   max_features = 'sqrt', verbose = True)\n",
    "# pipeline = make_pipeline(select, clf)\n",
    "# pipeline.fit(X_test, y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#veclyrics pkl\n",
    "#response pkl\n",
    "#try tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    0.9s remaining:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "                       oob_score=False, random_state=None, verbose=True,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting random forest classifier\n",
    "#works for multilabel response data by default\n",
    "rf = RandomForestClassifier(verbose = True, n_jobs= -1)\n",
    "rf.fit(X_train, y_train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7467970660146699, 0.8121271393643031, 0.5490464547677262, 0.8497799511002445, 0.7767237163814181, 0.8080195599022005, 0.9282151589242054]\n",
      "rock                 0.759511\n",
      "singer-songwriter    0.178289\n",
      "pop                  0.446455\n",
      "metal                0.149438\n",
      "folk                 0.207335\n",
      "country              0.183081\n",
      "hip hop / rap        0.072665\n",
      "dtype: float64\n",
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85      7766\n",
      "           1       0.19      0.02      0.03      1823\n",
      "           2       0.49      0.27      0.35      4565\n",
      "           3       0.35      0.01      0.01      1528\n",
      "           4       0.22      0.03      0.05      2120\n",
      "           5       0.25      0.02      0.04      1872\n",
      "           6       0.65      0.03      0.05       743\n",
      "\n",
      "   micro avg       0.69      0.42      0.53     20417\n",
      "   macro avg       0.42      0.19      0.20     20417\n",
      "weighted avg       0.52      0.42      0.42     20417\n",
      " samples avg       0.70      0.47      0.52     20417\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#y_pred_rf vs y_test\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "acc = []\n",
    "\n",
    "for i in range(0,7):\n",
    "    check = y_pred_rf[:,i] == np.array(y_test_all.iloc[:,i])\n",
    "    accuracy = check.sum()/len(check)\n",
    "    acc.append(accuracy)\n",
    "\n",
    "#prints raw accuracy score by genre\n",
    "print(acc)\n",
    "\n",
    "#average accuracy score\n",
    "sum(acc)/len(acc)\n",
    "\n",
    "#proportion of 1s in each genre column\n",
    "print(y_test_all.sum()/len(y_test_all))\n",
    "\n",
    "#why is classification report not matching (anywhere) the manually calculated raw accuracies?\n",
    "print('test report',classification_report(y_test_all, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1710513447432763"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appears to only be correct when whole vector of predictions is correct.  our metrics should check genre by genre.\n",
    "rf.score(X_test,y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85      7766\n",
      "           1       0.19      0.02      0.03      1823\n",
      "           2       0.49      0.27      0.35      4565\n",
      "           3       0.35      0.01      0.01      1528\n",
      "           4       0.22      0.03      0.05      2120\n",
      "           5       0.25      0.02      0.04      1872\n",
      "           6       0.65      0.03      0.05       743\n",
      "\n",
      "   micro avg       0.69      0.42      0.53     20417\n",
      "   macro avg       0.42      0.19      0.20     20417\n",
      "weighted avg       0.52      0.42      0.42     20417\n",
      " samples avg       0.70      0.47      0.52     20417\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     30708\n",
      "           1       1.00      0.88      0.93      7281\n",
      "           2       1.00      0.97      0.98     18372\n",
      "           3       1.00      0.87      0.93      6221\n",
      "           4       1.00      0.90      0.94      8429\n",
      "           5       1.00      0.89      0.94      7519\n",
      "           6       1.00      0.90      0.95      3216\n",
      "\n",
      "   micro avg       1.00      0.95      0.97     81746\n",
      "   macro avg       1.00      0.91      0.95     81746\n",
      "weighted avg       1.00      0.95      0.97     81746\n",
      " samples avg       0.99      0.96      0.96     81746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "print('test report',classification_report(y_test_all, y_pred_rf))\n",
    "print('train report',classification_report(y_train_all, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doesn't know n_jobs\n",
    "#won't take multilabel by default\n",
    "#doesn't seem to be working\n",
    "\n",
    "#svc = SVC(probability=True, verbose = True)\n",
    "#svc.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test_one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
