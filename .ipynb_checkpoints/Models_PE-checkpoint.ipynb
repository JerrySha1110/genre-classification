{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/11/cba4be5a737c6431323b89b5ade818b3bbe1df6e8261c6c70221a767c5d9/xgboost-1.0.2-py3-none-win_amd64.whl (24.6MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.16.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\peter\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.0.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "#!pip install scikit-multilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "# from better_profanity import profanity\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim import utils\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge #import ridge \n",
    "\n",
    "#imports\n",
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "\n",
    "#doc2vec\n",
    "X_train_vec = pd.read_pickle('veclyrics_doc2vec_train.pkl')\n",
    "X_test_vec = pd.read_pickle('veclyrics_doc2vec_test.pkl')\n",
    "y_train_vec = pd.read_pickle('response_doc2vec_train.pkl')\n",
    "y_test_vec = pd.read_pickle('response_doc2vec_test.pkl')\n",
    "y_train_vec = np.array(y_train_vec.iloc[:,3:10])\n",
    "y_test_vec = np.array(y_test_vec.iloc[:,3:10])\n",
    "\n",
    "y_train_vec1 = y_train_all_vec[:,0]\n",
    "y_test_vec1 = y_test_all_vec[:,0]\n",
    "y_train_vec2 = y_train_all_vec[:,1]\n",
    "y_test_vec2 = y_test_all_vec[:,1]\n",
    "y_train_vec3 = y_train_all_vec[:,2]\n",
    "y_test_vec3 = y_test_all_vec[:,2]\n",
    "y_train_vec4 = y_train_all_vec[:,3]\n",
    "y_test_vec4 = y_test_all_vec[:,3]\n",
    "y_train_vec5 = y_train_all_vec[:,4]\n",
    "y_test_vec5 = y_test_all_vec[:,4]\n",
    "y_train_vec6 = y_train_all_vec[:,5]\n",
    "y_test_vec6 = y_test_all_vec[:,5]\n",
    "y_train_vec7 = y_train_all_vec[:,6]\n",
    "y_test_vec7 = y_test_all_vec[:,6]\n",
    "\n",
    "#pre-split tfidf\n",
    "X_train_tf = pd.read_pickle('veclyrics_tfidf_train.pkl')\n",
    "X_test_tf = pd.read_pickle('veclyrics_tfidf_test.pkl')\n",
    "y_train_all_tf = pd.read_pickle('response_tfidf_train.pkl')\n",
    "y_test_all_tf = pd.read_pickle('response_tfidf_test.pkl')\n",
    "y_train_one_tf = y_train_all_tf.iloc[:,0]\n",
    "y_test_one_tf = y_test_all_tf.iloc[:,0]\n",
    "y_train_all_tf = np.array(y_train_all_tf.iloc[:,3:10])\n",
    "y_test_all_tf = np.array(y_test_all_tf.iloc[:,3:10])\n",
    "\n",
    "#original tfidf\n",
    "veclyrics = pd.read_pickle('veclyrics.pkl')\n",
    "response = np.array(pd.read_pickle('response.pkl'))[:,3:10]\n",
    "X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(veclyrics, response, test_size=0.2, random_state=0)\n",
    "y_train_tf = y_train_tf.astype(int)\n",
    "\n",
    "orgresponse = pd.read_pickle('response.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10225, 22079)\n",
      "(40897, 42176)\n"
     ]
    }
   ],
   "source": [
    "#pre-split tfidf won't work\n",
    "print(X_test_tf.shape)\n",
    "print(X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal for today: apply gb and log reg with loops across whole response set\n",
    "\n",
    "default parameters on gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pred):\n",
    "    \n",
    "    #pred (function input) is array of predictions made using X test\n",
    "    #y_test_vec (imported elsewhere) is array of y test\n",
    "    \n",
    "    acc = []\n",
    "\n",
    "    for i in range(0,7):\n",
    "        check = np.array(pred[:,i]) == y_test_vec[:,i]\n",
    "        accuracy = check.sum()/len(check)\n",
    "        acc.append(accuracy)\n",
    "    \n",
    "    null = pd.DataFrame(pd.DataFrame(y_test_vec).sum()/len(pd.DataFrame(y_test_vec)))\n",
    "    null.iloc[1:,0] = 1-null.iloc[1:,0]\n",
    "    \n",
    "    comp = pd.DataFrame({'accuracy':list(acc), 'null':list(null[0])})\n",
    "    comp['diff'] = comp['accuracy'] - comp['null']\n",
    "    \n",
    "    return(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1026           42.07s\n",
      "         2           1.0875           41.69s\n",
      "         3           1.0753           41.12s\n",
      "         4           1.0650           41.02s\n",
      "         5           1.0563           40.28s\n",
      "         6           1.0484           39.92s\n",
      "         7           1.0413           39.73s\n",
      "         8           1.0359           39.29s\n",
      "         9           1.0302           39.09s\n",
      "        10           1.0255           38.75s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-fccb8ec8b85c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgb_vec1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[0;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1546\u001b[1;33m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1608\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[0;32m   1609\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1242\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1244\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialize models\n",
    "gb_vec1 = GradientBoostingClassifier()\n",
    "gb_vec2 = GradientBoostingClassifier()\n",
    "gb_vec3 = GradientBoostingClassifier()\n",
    "gb_vec4 = GradientBoostingClassifier()\n",
    "gb_vec5 = GradientBoostingClassifier()\n",
    "gb_vec6 = GradientBoostingClassifier()\n",
    "gb_vec7 = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit models\n",
    "# ~5 min runtime\n",
    "\n",
    "gb1 = GradientBoostingClassifier()\n",
    "gb1.fit(X_train_vec,y_train_vec1)\n",
    "\n",
    "gb2 = GradientBoostingClassifier()\n",
    "gb2.fit(X_train_vec,y_train_vec2)\n",
    "\n",
    "gb3 = GradientBoostingClassifier()\n",
    "gb3.fit(X_train_vec,y_train_vec3)\n",
    "\n",
    "gb4 = GradientBoostingClassifier()\n",
    "gb4.fit(X_train_vec,y_train_vec4)\n",
    "\n",
    "gb5 = GradientBoostingClassifier()\n",
    "gb5.fit(X_train_vec,y_train_vec5)\n",
    "\n",
    "gb6 = GradientBoostingClassifier()\n",
    "gb6.fit(X_train_vec,y_train_vec6)\n",
    "\n",
    "gb7 = GradientBoostingClassifier()\n",
    "gb7.fit(X_train_vec,y_train_vec7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.768020</td>\n",
       "      <td>0.759511</td>\n",
       "      <td>0.008509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.819951</td>\n",
       "      <td>0.821711</td>\n",
       "      <td>-0.001760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571932</td>\n",
       "      <td>0.553545</td>\n",
       "      <td>0.018386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851149</td>\n",
       "      <td>0.850562</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.789633</td>\n",
       "      <td>0.792665</td>\n",
       "      <td>-0.003032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.814474</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>-0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.932225</td>\n",
       "      <td>0.927335</td>\n",
       "      <td>0.004890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.768020  0.759511  0.008509\n",
       "1  0.819951  0.821711 -0.001760\n",
       "2  0.571932  0.553545  0.018386\n",
       "3  0.851149  0.850562  0.000587\n",
       "4  0.789633  0.792665 -0.003032\n",
       "5  0.814474  0.816919 -0.002445\n",
       "6  0.932225  0.927335  0.004890"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test and evaluate models\n",
    "gb1pred = gb1.predict(X_test_vec)\n",
    "gb2pred = gb2.predict(X_test_vec)\n",
    "gb3pred = gb3.predict(X_test_vec)\n",
    "gb4pred = gb4.predict(X_test_vec)\n",
    "gb5pred = gb5.predict(X_test_vec)\n",
    "gb6pred = gb6.predict(X_test_vec)\n",
    "gb7pred = gb7.predict(X_test_vec)\n",
    "\n",
    "gb_pred_vec = pd.DataFrame({'rock':gb1pred, 'singer-songwriter':gb2pred, 'pop':gb3pred, \n",
    "              'metal':gb4pred, 'folk':gb5pred, 'country':gb6pred, 'hip hop / rap':gb7pred})\n",
    "\n",
    "gb_pred_vec = np.array(gb_pred_vec)\n",
    "\n",
    "eval(gb_pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  81 | elapsed:  4.1min remaining:   19.6s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7985231551665118"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search Gradient boosting classifier\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=3)\n",
    "# params_gbc = {\n",
    "#     \"learning_rate\": [0.01, 0.05, 0.10, 0.15],\n",
    "#     \"max_depth\": [3, 4, 5, 6],\n",
    "#     \"min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "#     \"max_features\":['sqrt']\n",
    "# }\n",
    "\n",
    "params_gbc = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.10],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_leaf\": [1, 2, 3],\n",
    "    \"max_features\":['sqrt']\n",
    "}\n",
    "\n",
    "#reducing from 10 with kfold to cv = 5 to halve fits\n",
    "gsgbc = GridSearchCV(gb, param_grid=params_gbc, cv=3, scoring=\"accuracy\", n_jobs= -1,verbose = 10)\n",
    "\n",
    "#gb_best = gsgbc.best_estimator_\n",
    "\n",
    "gsgbc.fit(X_train,y_train_one)\n",
    "\n",
    "# Best score\n",
    "gsgbc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7699755501222494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.score(X_test,y_test_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rockparam = gsgbc.best_estimator_\n",
    "rockpred = rockparam.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.13      0.21      2459\n",
      "           1       0.78      0.97      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.69      0.55      0.54     10225\n",
      "weighted avg       0.74      0.77      0.71     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.36      0.52     10190\n",
      "           1       0.82      0.99      0.90     30708\n",
      "\n",
      "    accuracy                           0.83     40898\n",
      "   macro avg       0.87      0.67      0.71     40898\n",
      "weighted avg       0.85      0.83      0.80     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rockparam.predict(X_test)\n",
    "y_pred_train_rf = rockparam.predict(X_train)\n",
    "print('test report',classification_report(y_test_one, y_pred_rf))\n",
    "print('train report',classification_report(y_train_one, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([11.3081518 , 13.08590802, 13.12594668, 18.31042401, 19.43094103,\n",
       "        20.38704999, 27.49654754, 27.80691083, 28.40115023, 15.38440625,\n",
       "        13.60730775, 11.39607056, 17.16316581, 19.0936594 , 19.81371172,\n",
       "        26.59359026, 28.10804582, 28.44664598, 16.39201482, 12.78171674,\n",
       "        10.58074252, 15.82891202, 17.96121391, 19.3528076 , 27.18937818,\n",
       "        25.85446143, 19.18196519]),\n",
       " 'std_fit_time': array([1.26626   , 0.27484841, 0.23021418, 0.63289025, 0.45235387,\n",
       "        0.38011948, 0.87657483, 0.86250159, 1.3267893 , 0.42033844,\n",
       "        0.75131222, 0.21753072, 0.23068524, 0.82817441, 0.25773188,\n",
       "        0.39976925, 1.24255933, 2.07529208, 0.48978439, 0.97744221,\n",
       "        0.21217617, 0.91924774, 0.60534562, 0.33500888, 0.83075881,\n",
       "        1.36594184, 3.06516744]),\n",
       " 'mean_score_time': array([0.13264505, 0.12400134, 0.12765892, 0.18417533, 0.15658085,\n",
       "        0.16954732, 0.21841685, 0.20811049, 0.1863389 , 0.14062421,\n",
       "        0.12765892, 0.11968017, 0.16023898, 0.16090377, 0.15957395,\n",
       "        0.1855046 , 0.20677956, 0.22373478, 0.13946613, 0.1160237 ,\n",
       "        0.11402957, 0.1522603 , 0.14760574, 0.17320371, 0.20079748,\n",
       "        0.12998517, 0.09607641]),\n",
       " 'std_score_time': array([0.00354908, 0.00448522, 0.00141153, 0.02125574, 0.00880809,\n",
       "        0.02327671, 0.01976301, 0.02644164, 0.00834299, 0.00776848,\n",
       "        0.00635987, 0.01345403, 0.00554215, 0.0061124 , 0.00850226,\n",
       "        0.00423132, 0.00463126, 0.04652488, 0.01916016, 0.00971459,\n",
       "        0.00803388, 0.01011686, 0.00293682, 0.0126512 , 0.01579752,\n",
       "        0.01339093, 0.00542269]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 4, 4, 4, 5, 5, 5, 3, 3, 3, 4, 4, 4, 5, 5, 5,\n",
       "                    3, 3, 3, 4, 4, 4, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3,\n",
       "                    1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.01,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.05,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2},\n",
       "  {'learning_rate': 0.1,\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 3}],\n",
       " 'split0_test_score': array([0.78302648, 0.78302648, 0.78302648, 0.786474  , 0.786474  ,\n",
       "        0.78654735, 0.7893347 , 0.78992151, 0.78977481, 0.79571628,\n",
       "        0.79615639, 0.79608303, 0.79593633, 0.79564293, 0.79666985,\n",
       "        0.79622974, 0.7970366 , 0.7968899 , 0.79454265, 0.79454265,\n",
       "        0.79578963, 0.79659649, 0.79615639, 0.79542287, 0.79527617,\n",
       "        0.79710995, 0.79454265]),\n",
       " 'split1_test_score': array([0.78251302, 0.78251302, 0.78251302, 0.78581383, 0.78588719,\n",
       "        0.78574048, 0.78940805, 0.78955476, 0.78926135, 0.79505611,\n",
       "        0.79556957, 0.79586298, 0.79835693, 0.79784347, 0.79806352,\n",
       "        0.79923715, 0.79967725, 0.79894374, 0.79828358, 0.79762341,\n",
       "        0.7993105 , 0.79710995, 0.79791682, 0.7994572 , 0.79843028,\n",
       "        0.79989731, 0.79799017]),\n",
       " 'split2_test_score': array([0.78249707, 0.78249707, 0.78249707, 0.78543134, 0.78587148,\n",
       "        0.78543134, 0.78851232, 0.78880575, 0.78873239, 0.79401408,\n",
       "        0.79401408, 0.79350059, 0.797902  , 0.79650822, 0.79643486,\n",
       "        0.79768192, 0.79775528, 0.79665493, 0.79504108, 0.79496772,\n",
       "        0.79533451, 0.7963615 , 0.79702171, 0.79768192, 0.79702171,\n",
       "        0.79856221, 0.79746185]),\n",
       " 'mean_test_score': array([0.78267886, 0.78267886, 0.78267886, 0.7859064 , 0.78607756,\n",
       "        0.7859064 , 0.78908504, 0.78942736, 0.7892562 , 0.79492885,\n",
       "        0.79524671, 0.79514891, 0.79739841, 0.79666487, 0.79705609,\n",
       "        0.79771627, 0.79815639, 0.79749621, 0.79595579, 0.79571128,\n",
       "        0.79681158, 0.79668932, 0.79703164, 0.79752066, 0.79690938,\n",
       "        0.79852316, 0.79666487]),\n",
       " 'std_test_score': array([0.0002459 , 0.0002459 , 0.0002459 , 0.00043066, 0.0002804 ,\n",
       "        0.00047047, 0.00040606, 0.00046433, 0.00042558, 0.00070072,\n",
       "        0.00090389, 0.00116895, 0.00105041, 0.00090518, 0.00071881,\n",
       "        0.00122802, 0.00111474, 0.00102806, 0.00165855, 0.0013632 ,\n",
       "        0.00177677, 0.00031252, 0.00071874, 0.00165097, 0.00129012,\n",
       "        0.00113828, 0.00151608]),\n",
       " 'rank_test_score': array([25, 25, 25, 23, 22, 23, 21, 19, 20, 18, 16, 17,  6, 12,  7,  3,  2,\n",
       "         5, 14, 15, 10, 11,  8,  4,  9,  1, 12])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsgbc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 42 candidates, totalling 126 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gsrf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-21bcada65dd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgsrf_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'roc_auc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mgsrf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_one_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgsrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgsrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'gsrf' is not defined"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([('select',SelectKBest(k=20)), \n",
    "               ('classify', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n",
    "\n",
    "param_test = {'classify__n_estimators':list(range(20,50,5)), \n",
    "              'classify__max_depth':list(range(5,40,5))}\n",
    "gsrf_vec = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=3, verbose = True, n_jobs = -1)\n",
    "gsrf_vec.fit(X_train_vec,y_train_one_vec)\n",
    "print(gsrf_vec.best_params_, gsrf_vec.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.09      0.16      2459\n",
      "           1       0.77      0.99      0.87      7766\n",
      "\n",
      "    accuracy                           0.77     10225\n",
      "   macro avg       0.75      0.54      0.51     10225\n",
      "weighted avg       0.76      0.77      0.70     10225\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.27      0.43     10190\n",
      "           1       0.81      1.00      0.89     30708\n",
      "\n",
      "    accuracy                           0.82     40898\n",
      "   macro avg       0.89      0.64      0.66     40898\n",
      "weighted avg       0.85      0.82      0.78     40898\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test and train reports appear to be somehow flipped?\n",
    "#regardless this is not that good\n",
    "y_pred_gsrf_vec = gsrf_vec.predict(X_test_vec)\n",
    "y_pred_train_gsrf_vec = gsrf_vec.predict(X_train_vec)\n",
    "print('test report',classification_report(y_test_one_vec, y_pred_gsrf_vec))\n",
    "print('train report',classification_report(y_train_one_vec, y_pred_train_gsrf_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_gsrf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    2.0s remaining:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "                       oob_score=False, random_state=None, verbose=True,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting random forest classifier\n",
    "#works for multilabel response data by default\n",
    "rf_vec = RandomForestClassifier(verbose = True, n_jobs= -1)\n",
    "rf_vec.fit(X_train_vec, y_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "rf_pred_vec = rf_vec.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>null</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.744743</td>\n",
       "      <td>0.759511</td>\n",
       "      <td>-0.014768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.814181</td>\n",
       "      <td>0.821711</td>\n",
       "      <td>-0.007531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.552665</td>\n",
       "      <td>0.553545</td>\n",
       "      <td>-0.000880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.849095</td>\n",
       "      <td>0.850562</td>\n",
       "      <td>-0.001467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.781809</td>\n",
       "      <td>0.792665</td>\n",
       "      <td>-0.010856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.811149</td>\n",
       "      <td>0.816919</td>\n",
       "      <td>-0.005770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.928411</td>\n",
       "      <td>0.927335</td>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy      null      diff\n",
       "0  0.744743  0.759511 -0.014768\n",
       "1  0.814181  0.821711 -0.007531\n",
       "2  0.552665  0.553545 -0.000880\n",
       "3  0.849095  0.850562 -0.001467\n",
       "4  0.781809  0.792665 -0.010856\n",
       "5  0.811149  0.816919 -0.005770\n",
       "6  0.928411  0.927335  0.001076"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(rf_pred_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "C:\\Users\\peter\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning:\n",
      "\n",
      "Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85      7766\n",
      "           1       0.19      0.02      0.03      1823\n",
      "           2       0.49      0.27      0.35      4565\n",
      "           3       0.35      0.01      0.01      1528\n",
      "           4       0.22      0.03      0.05      2120\n",
      "           5       0.25      0.02      0.04      1872\n",
      "           6       0.65      0.03      0.05       743\n",
      "\n",
      "   micro avg       0.69      0.42      0.53     20417\n",
      "   macro avg       0.42      0.19      0.20     20417\n",
      "weighted avg       0.52      0.42      0.42     20417\n",
      " samples avg       0.70      0.47      0.52     20417\n",
      "\n",
      "train report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     30708\n",
      "           1       1.00      0.88      0.93      7281\n",
      "           2       1.00      0.97      0.98     18372\n",
      "           3       1.00      0.87      0.93      6221\n",
      "           4       1.00      0.90      0.94      8429\n",
      "           5       1.00      0.89      0.94      7519\n",
      "           6       1.00      0.90      0.95      3216\n",
      "\n",
      "   micro avg       1.00      0.95      0.97     81746\n",
      "   macro avg       1.00      0.91      0.95     81746\n",
      "weighted avg       1.00      0.95      0.97     81746\n",
      " samples avg       0.99      0.96      0.96     81746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#how i would normally check an individual response class\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_train_rf = rf.predict(X_train)\n",
    "print('test report',classification_report(y_test_all, y_pred_rf))\n",
    "print('train report',classification_report(y_train_all, y_pred_train_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test,y_test_one)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
