{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
      "Requirement already satisfied: six in c:\\users\\peter\\anaconda3\\lib\\site-packages (from langdetect) (1.12.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\peter\\AppData\\Local\\pip\\Cache\\wheels\\ec\\0c\\a9\\1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.7\n"
     ]
    }
   ],
   "source": [
    "#!pip install textblob\n",
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "songdata = pd.read_csv('./songdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./genres.pkl\")\n",
    "df = pd.merge(songdata, df, on=['artist','artist'])\n",
    "df = df[df.iloc[:,5:].any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>genres</th>\n",
       "      <th>rock</th>\n",
       "      <th>singer-songwriter</th>\n",
       "      <th>pop</th>\n",
       "      <th>metal</th>\n",
       "      <th>folk</th>\n",
       "      <th>country</th>\n",
       "      <th>hip hop / rap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>[europop, swedish pop]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>[europop, swedish pop]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>[europop, swedish pop]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[europop, swedish pop]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>[europop, swedish pop]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text                  genres  \\\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  [europop, swedish pop]   \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  [europop, swedish pop]   \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  [europop, swedish pop]   \n",
       "3  Making somebody happy is a question of give an...  [europop, swedish pop]   \n",
       "4  Making somebody happy is a question of give an...  [europop, swedish pop]   \n",
       "\n",
       "   rock  singer-songwriter  pop  metal  folk  country  hip hop / rap  \n",
       "0     0                  0    1      0     0        0              0  \n",
       "1     0                  0    1      0     0        0              0  \n",
       "2     0                  0    1      0     0        0              0  \n",
       "3     0                  0    1      0     0        0              0  \n",
       "4     0                  0    1      0     0        0              0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51489 entries, 0 to 57595\n",
      "Data columns (total 12 columns):\n",
      "artist               51489 non-null object\n",
      "song                 51489 non-null object\n",
      "link                 51489 non-null object\n",
      "text                 51489 non-null object\n",
      "genres               51489 non-null object\n",
      "rock                 51489 non-null int32\n",
      "singer-songwriter    51489 non-null int32\n",
      "pop                  51489 non-null int32\n",
      "metal                51489 non-null int32\n",
      "folk                 51489 non-null int32\n",
      "country              51489 non-null int32\n",
      "hip hop / rap        51489 non-null int32\n",
      "dtypes: int32(7), object(5)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a multilabel classification problem according to https://scikit-learn.org/stable/modules/multiclass.html\n",
    "\n",
    "Multilabel- \n",
    "\n",
    "\"A multiclass multioutput target where each output is binary. This may be represented as a 2d (dense) array or sparse matrix of integers, such that each column is a separate binary target, where positive labels are indicated with 1 and negative labels are usually -1 or 0. Sparse multilabel targets are not supported everywhere that dense multilabel targets are supported.\"\n",
    "\n",
    "\"Valid representation of multilabel y is either dense (or sparse) binary matrix of shape (n_samples, n_classes). Each column represents a class. The 1’s in each row denote the positive classes a sample has been labelled with.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here is y in array form if you want it.\n",
    "#this would have happened in a train_test_split anyway\n",
    "y = np.array(df.iloc[:,5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some text is in round brackets while some is in square brackets. I wanted to examine what that text looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63285\n"
     ]
    }
   ],
   "source": [
    "#examining text in round brackets\n",
    "round_brackets = sum(list(df['text'].map(lambda s: re.findall(r'\\((.*?)\\)',s))), [])\n",
    "#Number of round brackets:\n",
    "print((len(round_brackets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful',\n",
       " 'oh aah',\n",
       " 'no no',\n",
       " 'For sure',\n",
       " 'Dolly Parton',\n",
       " 'oo',\n",
       " 'Still in love',\n",
       " 'go Low',\n",
       " 'provoke',\n",
       " 'No batteries required',\n",
       " 'from the DP',\n",
       " 'hey',\n",
       " '2x',\n",
       " 'Got to be there',\n",
       " 'my boys']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Viewing some text in round brackets\n",
    "#These just look like normal lyrics\n",
    "random.seed(0)\n",
    "random.choices(round_brackets, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29009\n"
     ]
    }
   ],
   "source": [
    "#examining text in square brackets\n",
    "square_brackets = sum(list(df['text'].map(lambda s: re.findall(r'\\[(.*?)\\]',s))), [])\n",
    "#how many instances of square brackets?\n",
    "print((len(square_brackets)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub(pattern, repl, string, count=0, flags=0)\n",
    "# remove round brackets but not text within\n",
    "df['text'] = df['text'].map(lambda s: re.sub(r'\\(|\\)', '', s))\n",
    "\n",
    "# remove square brackest and text within\n",
    "df['text'] = df['text'].map(lambda s: re.sub(r'\\[(.*?)\\] ', '', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove line breaks\n",
    "df['text'] = df['text'].map(lambda s: re.sub(r' \\n|\\n', '', s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Non-English Songs\n",
    "\n",
    "Note: This takes a while to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs in english: 57175\n",
      "Number of songs that are not english: 475\n"
     ]
    }
   ],
   "source": [
    "#This find the probability that the word is english\n",
    "def get_eng_prob(text):\n",
    "    detections = detect_langs(text)\n",
    "    for detection in detections:\n",
    "        if detection.lang == 'en':\n",
    "            return detection.prob\n",
    "    return 0\n",
    "\n",
    "#finding the probability that the text is english\n",
    "df['en_prob'] = df['text'].map(get_eng_prob)\n",
    "\n",
    "print('Number of songs in english: {}'.format(sum(df['en_prob'] >= 0.5)))\n",
    "print('Number of songs that are not english: {}'.format(sum(df['en_prob'] < 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only selecting songs that have a probability of 0.5 or higher of being in english\n",
    "df = df.loc[df['en_prob'] >= 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>en_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>0.999995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text   en_prob  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  0.999996  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  0.999995  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  0.999996  \n",
       "3  Making somebody happy is a question of give an...  0.999996  \n",
       "4  Making somebody happy is a question of give an...  0.999997  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of songs in english: 57180\n",
      "Number of songs that are not english: 0\n"
     ]
    }
   ],
   "source": [
    "#Repeating above code to make sure that the non-english songs were dropped\n",
    "print('Number of songs in english: {}'.format(sum(df['en_prob'] >= 0.5)))\n",
    "print('Number of songs that are not english: {}'.format(sum(df['en_prob'] < 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['en_prob'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Using nltk.tokenize to seperate the text into a list of words. All punctuation is removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w+ for whitespace\n",
    "#tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#creating a new column called tokens\n",
    "#df['tokens'] = df['text'].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We originally cleaned the data using the tokenization text above. After later running the word2vec, we didn't like that the words were not lowercased, and thought they could be cleaned up better. We are proceeding with the code below instead. According to documentation, this uses tokenize() internally.\n",
    "\n",
    "https://radimrehurek.com/gensim/utils.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def lemmatize_stemming(text):\n",
    "    \"\"\"\n",
    "       Also Borrowed from our preprocessing module.\n",
    "    \"\"\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "        Edited function from our Preprocessing pipeline\n",
    "        Doesn't remove stopwords as it turns out some stopwords like 'NO' is actually very importan\n",
    "    \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    #stopwords = list(gensim.parsing.preprocessing.STOPWORDS)\n",
    "    #stopwords.pop(stopwords.index('no'))\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        #if token not in set(stopwords):\n",
    "        result.append(lemmatize_stemming(token))\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_preprocessed'] = df['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>look at her face it wonder face and it mean so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>take it easi with me pleas touch me gentli lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>ll never know whi have to go whi have to put u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...   \n",
       "1  Take it easy with me, please  \\nTouch me gentl...   \n",
       "2  I'll never know why I had to go  \\nWhy I had t...   \n",
       "3  Making somebody happy is a question of give an...   \n",
       "4  Making somebody happy is a question of give an...   \n",
       "\n",
       "                                   text_preprocessed  \n",
       "0  look at her face it wonder face and it mean so...  \n",
       "1  take it easi with me pleas touch me gentli lik...  \n",
       "2  ll never know whi have to go whi have to put u...  \n",
       "3  make somebodi happi be question of give and ta...  \n",
       "4  make somebodi happi be question of give and ta...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    look at her face it wonder face and it mean so...\n",
       "1    take it easi with me pleas touch me gentli lik...\n",
       "2    ll never know whi have to go whi have to put u...\n",
       "3    make somebodi happi be question of give and ta...\n",
       "4    make somebodi happi be question of give and ta...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking what the tokens column look like before removing stopwords\n",
    "df['text_preprocessed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safety\n",
    "df2 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w+ for whitespace\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#creating a new column called tokens\n",
    "df['tokens'] = df['text_preprocessed'].map(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>look at her face it wonder face and it mean so...</td>\n",
       "      <td>[look, at, her, face, it, wonder, face, and, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>take it easi with me pleas touch me gentli lik...</td>\n",
       "      <td>[take, it, easi, with, me, pleas, touch, me, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>ll never know whi have to go whi have to put u...</td>\n",
       "      <td>[ll, never, know, whi, have, to, go, whi, have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, be, question, of, give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, be, question, of, give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57645</th>\n",
       "      <td>Ziggy Marley</td>\n",
       "      <td>Good Old Days</td>\n",
       "      <td>/z/ziggy+marley/good+old+days_10198588.html</td>\n",
       "      <td>Irie days come on play  \\nLet the angels fly l...</td>\n",
       "      <td>iri day come on play let the angel fli let the...</td>\n",
       "      <td>[iri, day, come, on, play, let, the, angel, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57646</th>\n",
       "      <td>Ziggy Marley</td>\n",
       "      <td>Hand To Mouth</td>\n",
       "      <td>/z/ziggy+marley/hand+to+mouth_20531167.html</td>\n",
       "      <td>Power to the workers  \\nMore power  \\nPower to...</td>\n",
       "      <td>power to the worker more power power to the wo...</td>\n",
       "      <td>[power, to, the, worker, more, power, power, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57647</th>\n",
       "      <td>Zwan</td>\n",
       "      <td>Come With Me</td>\n",
       "      <td>/z/zwan/come+with+me_20148981.html</td>\n",
       "      <td>all you need  \\nis something i'll believe  \\nf...</td>\n",
       "      <td>all you need be someth ll believ flashlight in...</td>\n",
       "      <td>[all, you, need, be, someth, ll, believ, flash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57648</th>\n",
       "      <td>Zwan</td>\n",
       "      <td>Desire</td>\n",
       "      <td>/z/zwan/desire_20148986.html</td>\n",
       "      <td>northern star  \\nam i frightened  \\nwhere can ...</td>\n",
       "      <td>northern star be frighten where can go to rest...</td>\n",
       "      <td>[northern, star, be, frighten, where, can, go,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57649</th>\n",
       "      <td>Zwan</td>\n",
       "      <td>Heartsong</td>\n",
       "      <td>/z/zwan/heartsong_20148991.html</td>\n",
       "      <td>come in  \\nmake yourself at home  \\ni'm a bit ...</td>\n",
       "      <td>come in make yourself at home bite late hate t...</td>\n",
       "      <td>[come, in, make, yourself, at, home, bite, lat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57175 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             artist                   song  \\\n",
       "0              ABBA  Ahe's My Kind Of Girl   \n",
       "1              ABBA       Andante, Andante   \n",
       "2              ABBA         As Good As New   \n",
       "3              ABBA                   Bang   \n",
       "4              ABBA       Bang-A-Boomerang   \n",
       "...             ...                    ...   \n",
       "57645  Ziggy Marley          Good Old Days   \n",
       "57646  Ziggy Marley          Hand To Mouth   \n",
       "57647          Zwan           Come With Me   \n",
       "57648          Zwan                 Desire   \n",
       "57649          Zwan              Heartsong   \n",
       "\n",
       "                                              link  \\\n",
       "0       /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1            /a/abba/andante+andante_20002708.html   \n",
       "2             /a/abba/as+good+as+new_20003033.html   \n",
       "3                       /a/abba/bang_20598415.html   \n",
       "4           /a/abba/bang+a+boomerang_20002668.html   \n",
       "...                                            ...   \n",
       "57645  /z/ziggy+marley/good+old+days_10198588.html   \n",
       "57646  /z/ziggy+marley/hand+to+mouth_20531167.html   \n",
       "57647           /z/zwan/come+with+me_20148981.html   \n",
       "57648                 /z/zwan/desire_20148986.html   \n",
       "57649              /z/zwan/heartsong_20148991.html   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Look at her face, it's a wonderful face  \\nAnd...   \n",
       "1      Take it easy with me, please  \\nTouch me gentl...   \n",
       "2      I'll never know why I had to go  \\nWhy I had t...   \n",
       "3      Making somebody happy is a question of give an...   \n",
       "4      Making somebody happy is a question of give an...   \n",
       "...                                                  ...   \n",
       "57645  Irie days come on play  \\nLet the angels fly l...   \n",
       "57646  Power to the workers  \\nMore power  \\nPower to...   \n",
       "57647  all you need  \\nis something i'll believe  \\nf...   \n",
       "57648  northern star  \\nam i frightened  \\nwhere can ...   \n",
       "57649  come in  \\nmake yourself at home  \\ni'm a bit ...   \n",
       "\n",
       "                                       text_preprocessed  \\\n",
       "0      look at her face it wonder face and it mean so...   \n",
       "1      take it easi with me pleas touch me gentli lik...   \n",
       "2      ll never know whi have to go whi have to put u...   \n",
       "3      make somebodi happi be question of give and ta...   \n",
       "4      make somebodi happi be question of give and ta...   \n",
       "...                                                  ...   \n",
       "57645  iri day come on play let the angel fli let the...   \n",
       "57646  power to the worker more power power to the wo...   \n",
       "57647  all you need be someth ll believ flashlight in...   \n",
       "57648  northern star be frighten where can go to rest...   \n",
       "57649  come in make yourself at home bite late hate t...   \n",
       "\n",
       "                                                  tokens  \n",
       "0      [look, at, her, face, it, wonder, face, and, i...  \n",
       "1      [take, it, easi, with, me, pleas, touch, me, g...  \n",
       "2      [ll, never, know, whi, have, to, go, whi, have...  \n",
       "3      [make, somebodi, happi, be, question, of, give...  \n",
       "4      [make, somebodi, happi, be, question, of, give...  \n",
       "...                                                  ...  \n",
       "57645  [iri, day, come, on, play, let, the, angel, fl...  \n",
       "57646  [power, to, the, worker, more, power, power, t...  \n",
       "57647  [all, you, need, be, someth, ll, believ, flash...  \n",
       "57648  [northern, star, be, frighten, where, can, go,...  \n",
       "57649  [come, in, make, yourself, at, home, bite, lat...  \n",
       "\n",
       "[57175 rows x 6 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "Reference for more info: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/breabeals/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#setting stop words\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [item for item in x if item not in stop_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>look at her face it wonder face and it mean so...</td>\n",
       "      <td>[look, face, wonder, face, mean, someth, speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>take it easi with me pleas touch me gentli lik...</td>\n",
       "      <td>[take, easi, pleas, touch, gentli, like, summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>ll never know whi have to go whi have to put u...</td>\n",
       "      <td>[never, know, whi, go, whi, put, lousi, rotten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...   \n",
       "1  Take it easy with me, please  \\nTouch me gentl...   \n",
       "2  I'll never know why I had to go  \\nWhy I had t...   \n",
       "3  Making somebody happy is a question of give an...   \n",
       "4  Making somebody happy is a question of give an...   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  look at her face it wonder face and it mean so...   \n",
       "1  take it easi with me pleas touch me gentli lik...   \n",
       "2  ll never know whi have to go whi have to put u...   \n",
       "3  make somebodi happi be question of give and ta...   \n",
       "4  make somebodi happi be question of give and ta...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [look, face, wonder, face, mean, someth, speci...  \n",
       "1  [take, easi, pleas, touch, gentli, like, summe...  \n",
       "2  [never, know, whi, go, whi, put, lousi, rotten...  \n",
       "3  [make, somebodi, happi, question, give, take, ...  \n",
       "4  [make, somebodi, happi, question, give, take, ...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the data - stemming done above in pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "# create dictionary to map tokens their lem\n",
    "token_to_lem = {}\n",
    "# initialise word count\n",
    "token_count = 0\n",
    "\n",
    "\n",
    "# iterate through all songs\n",
    "for lst in df['tokens']:\n",
    "    # iterate through all tokens of song\n",
    "    for token in lst:\n",
    "        token_count += 1\n",
    "        # check if token is in dictionary\n",
    "        if token not in token_to_lem:\n",
    "            # add token to dictionary\n",
    "            token_to_lem[token] = lemmatizer.lemmatize(token)\n",
    "    \n",
    "    \n",
    "#creating column stems            \n",
    "df['stems'] = df['tokens'].map(lambda lst: [token_to_lem[token] for token in lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the option to either lemmatization or stemming. We ran both and compared our outputs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#intialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# create dictionary to map tokens their stem\n",
    "token_to_stem = {}\n",
    "# initialise word count\n",
    "token_count = 0\n",
    "\n",
    "\n",
    "# iterate through all songs\n",
    "for lst in df['tokens']:\n",
    "    # iterate through all tokens of song\n",
    "    for token in lst:\n",
    "        token_count += 1\n",
    "        # check if token is in dictionary\n",
    "        if token not in token_to_stem:\n",
    "            # add token to dictionary\n",
    "            token_to_stem[token] = stemmer.stem(token)\n",
    "\n",
    "#creating column stems            \n",
    "df['stems'] = df['tokens'].map(lambda lst: [token_to_stem[token] for token in lst])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>look at her face it wonder face and it mean so...</td>\n",
       "      <td>[look, face, wonder, face, mean, someth, speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>take it easi with me pleas touch me gentli lik...</td>\n",
       "      <td>[take, easi, pleas, touch, gentli, like, summe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>ll never know whi have to go whi have to put u...</td>\n",
       "      <td>[never, know, whi, go, whi, put, lousi, rotten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...   \n",
       "1  Take it easy with me, please  \\nTouch me gentl...   \n",
       "2  I'll never know why I had to go  \\nWhy I had t...   \n",
       "3  Making somebody happy is a question of give an...   \n",
       "4  Making somebody happy is a question of give an...   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  look at her face it wonder face and it mean so...   \n",
       "1  take it easi with me pleas touch me gentli lik...   \n",
       "2  ll never know whi have to go whi have to put u...   \n",
       "3  make somebodi happi be question of give and ta...   \n",
       "4  make somebodi happi be question of give and ta...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [look, face, wonder, face, mean, someth, speci...  \n",
       "1  [take, easi, pleas, touch, gentli, like, summe...  \n",
       "2  [never, know, whi, go, whi, put, lousi, rotten...  \n",
       "3  [make, somebodi, happi, question, give, take, ...  \n",
       "4  [make, somebodi, happi, question, give, take, ...  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 57175 entries, 0 to 57649\n",
      "Data columns (total 7 columns):\n",
      "artist                       57175 non-null object\n",
      "song                         57175 non-null object\n",
      "link                         57175 non-null object\n",
      "text                         57175 non-null object\n",
      "text_preprocessed            57175 non-null object\n",
      "tokens                       57175 non-null object\n",
      "text_preprocessed_stopped    57175 non-null object\n",
      "dtypes: object(7)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "#checking out the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7746197\n"
     ]
    }
   ],
   "source": [
    "#number of tokens\n",
    "print(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102911"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique tokens\n",
    "len(token_to_stem.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57188"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique stems\n",
    "(len(set(token_to_stem.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the stems back into untokenized structrure so that we can run count vector on it\n",
    "df['stem_str'] = df['tokens'].map(lambda lst: ' '.join(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stem_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "      <td>look at her face it wonder face and it mean so...</td>\n",
       "      <td>[look, face, wonder, face, mean, someth, speci...</td>\n",
       "      <td>look face wonder face mean someth special look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "      <td>take it easi with me pleas touch me gentli lik...</td>\n",
       "      <td>[take, easi, pleas, touch, gentli, like, summe...</td>\n",
       "      <td>take easi pleas touch gentli like summer even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "      <td>ll never know whi have to go whi have to put u...</td>\n",
       "      <td>[never, know, whi, go, whi, put, lousi, rotten...</td>\n",
       "      <td>never know whi go whi put lousi rotten show bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "      <td>make somebodi happi question give take learn s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "      <td>make somebodi happi be question of give and ta...</td>\n",
       "      <td>[make, somebodi, happi, question, give, take, ...</td>\n",
       "      <td>make somebodi happi question give take learn s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \\\n",
       "0  Look at her face, it's a wonderful face  \\nAnd...   \n",
       "1  Take it easy with me, please  \\nTouch me gentl...   \n",
       "2  I'll never know why I had to go  \\nWhy I had t...   \n",
       "3  Making somebody happy is a question of give an...   \n",
       "4  Making somebody happy is a question of give an...   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  look at her face it wonder face and it mean so...   \n",
       "1  take it easi with me pleas touch me gentli lik...   \n",
       "2  ll never know whi have to go whi have to put u...   \n",
       "3  make somebodi happi be question of give and ta...   \n",
       "4  make somebodi happi be question of give and ta...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [look, face, wonder, face, mean, someth, speci...   \n",
       "1  [take, easi, pleas, touch, gentli, like, summe...   \n",
       "2  [never, know, whi, go, whi, put, lousi, rotten...   \n",
       "3  [make, somebodi, happi, question, give, take, ...   \n",
       "4  [make, somebodi, happi, question, give, take, ...   \n",
       "\n",
       "                                            stem_str  \n",
       "0  look face wonder face mean someth special look...  \n",
       "1  take easi pleas touch gentli like summer even ...  \n",
       "2  never know whi go whi put lousi rotten show bo...  \n",
       "3  make somebodi happi question give take learn s...  \n",
       "4  make somebodi happi question give take learn s...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Documentation: \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "\n",
    "# initialise count vectorizer\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "#This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "#cv = CountVectorizer()\n",
    "\n",
    "# generate word counts\n",
    "#stem_count_vector = cv.fit_transform(df['stem_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<57175x50762 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3334732 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stem_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM=300\n",
    "wv_model=Word2Vec(df[\"tokens\"],\n",
    "             size=EMB_DIM,\n",
    "             window=5,\n",
    "             min_count=5, \n",
    "             negative=15, \n",
    "             iter=10,\n",
    "             workers=multiprocessing.cpu_count(), \n",
    "             #defines the algorithm to use as the model. sg = skip-gram\n",
    "             #default is CBOW (Continuous bag of words)\n",
    "             sg = 1   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "import pickle\n",
    "  \n",
    "# Save the trained model as a pickle string. \n",
    "pickle.dump(wv_model, open( \"songs_wv_model\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors=wv_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring different types of word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lorain', 0.6302295923233032),\n",
       " ('couldv', 0.6193580627441406),\n",
       " ('distantli', 0.6171380877494812),\n",
       " ('overreact', 0.6133875250816345),\n",
       " ('unconvent', 0.6129610538482666),\n",
       " ('unmerit', 0.6109627485275269),\n",
       " ('questionin', 0.6030411720275879),\n",
       " ('connot', 0.6023432016372681),\n",
       " ('loophol', 0.5982478260993958),\n",
       " ('emigr', 0.5925590991973877)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploring similar words\n",
    "#calling object similar_by_word\n",
    "word_vectors.similar_by_word('masterplan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectors.most_similar(positive=['insertwordhere'],negative=['insertwordhere'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breabeals/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-2.24736318e-01,  1.97534621e-01, -4.29814868e-02,  5.71665466e-02,\n",
       "        5.65432794e-02,  1.85361847e-01, -6.85141310e-02,  8.89955685e-02,\n",
       "        1.21765053e-02,  2.06344709e-01, -1.14845835e-01, -1.25641763e-01,\n",
       "       -4.58884798e-03, -1.42729923e-03,  1.43025428e-01, -3.46773177e-01,\n",
       "        1.35805786e-01, -2.56192654e-01, -1.26809895e-01, -1.30737841e-01,\n",
       "        9.08707082e-02,  1.35844260e-01, -5.43559194e-02,  4.73242477e-02,\n",
       "        2.51561612e-01, -1.24103695e-01, -1.26703739e-01, -1.88817494e-02,\n",
       "        3.10138404e-01, -3.21486928e-02, -2.03293823e-02,  1.41334394e-02,\n",
       "       -1.42418832e-01, -6.57715797e-02, -1.12045988e-01,  1.60259008e-01,\n",
       "        1.85000822e-02,  1.49633542e-01, -8.46545473e-02, -9.50547680e-03,\n",
       "        5.91595583e-02, -1.16509996e-01,  4.31440026e-03,  3.29046361e-02,\n",
       "       -1.00284070e-01, -9.64479744e-02,  2.31143460e-01, -1.24456011e-01,\n",
       "        7.63623789e-02, -2.73124706e-02,  1.17235025e-02,  1.30105345e-03,\n",
       "        1.76290572e-01,  8.13145712e-02,  1.23784594e-01, -7.04271346e-02,\n",
       "        2.76564896e-01, -1.90152913e-01,  1.81408748e-01, -5.58746569e-02,\n",
       "        1.41019151e-01,  1.97566494e-01, -9.25799906e-02, -6.58866167e-02,\n",
       "       -1.69604504e-03,  3.28137465e-02, -3.38338539e-02, -2.56938040e-01,\n",
       "       -2.27604453e-02, -8.19249451e-02, -1.55938506e-01,  7.46764010e-03,\n",
       "        8.31638649e-02, -6.77590296e-02,  1.74724072e-01, -8.96205083e-02,\n",
       "       -1.28290430e-01, -2.56060734e-02, -8.19020867e-02, -1.87953681e-01,\n",
       "       -5.02296677e-03, -4.85777110e-02,  2.77173333e-02, -3.96085717e-02,\n",
       "        1.05949461e-01,  1.18659906e-01, -6.73286559e-04, -5.28888367e-02,\n",
       "        1.02058291e-01,  2.73336470e-01,  1.24527812e-01, -2.20185183e-02,\n",
       "        7.52717927e-02, -1.03098504e-01,  3.78298044e-01,  4.15418260e-02,\n",
       "       -6.71328380e-05, -1.48721009e-01, -2.36525796e-02, -1.27854183e-01,\n",
       "        2.24346384e-01,  1.48415625e-01,  1.44464150e-02,  1.36733741e-01,\n",
       "        2.09203184e-01,  2.38827839e-01, -1.13354199e-01, -7.02940673e-02,\n",
       "       -2.61263587e-02,  1.11787930e-01,  1.83259964e-01, -2.58659661e-01,\n",
       "       -9.67905223e-02,  1.55661162e-03,  3.66767198e-01, -2.07460493e-01,\n",
       "       -4.73914342e-03,  5.64589910e-02, -1.64229714e-03,  2.33604331e-02,\n",
       "        1.41441613e-01, -1.36170819e-01, -3.01247746e-01,  1.28419027e-01,\n",
       "        1.13408588e-01,  7.68720061e-02,  1.36228085e-01, -6.71759527e-03,\n",
       "       -2.82008708e-01,  1.76307023e-01,  2.79584020e-01,  1.28379717e-01,\n",
       "        7.11141378e-02, -3.27174515e-01,  1.05949387e-01, -1.11038178e-01,\n",
       "        6.86065406e-02, -5.20385876e-02,  2.18442217e-01, -1.28833130e-01,\n",
       "       -1.72818676e-01, -9.58222225e-02, -1.90981790e-01,  1.55857980e-01,\n",
       "       -1.43328123e-03,  1.21460222e-01,  1.81243539e-01, -1.06566556e-01,\n",
       "        1.91772968e-01, -1.60067737e-01, -1.17410114e-02, -3.31105180e-02,\n",
       "        1.91569597e-01, -7.35887289e-02,  3.57806869e-02, -2.80366153e-01,\n",
       "       -2.05583200e-02,  7.43774548e-02,  5.96090965e-02, -3.15311477e-02,\n",
       "        9.77665186e-03,  1.96030512e-01, -3.07896227e-01, -4.02413793e-02,\n",
       "       -1.81018084e-01,  4.79842983e-02,  2.18753800e-01, -1.97936982e-01,\n",
       "        2.32451886e-01,  7.38897501e-03,  3.53122167e-02,  4.48937453e-02,\n",
       "        2.80990750e-02,  7.95468688e-02, -1.08018562e-01, -4.66212779e-02,\n",
       "       -6.41077533e-02, -1.81254894e-01,  1.41713426e-01,  5.52472174e-02,\n",
       "        2.25181449e-02, -4.19314690e-02, -6.22941703e-02,  5.43500148e-02,\n",
       "       -1.85167149e-01,  4.09301892e-02, -6.55594990e-02, -1.78816155e-01,\n",
       "        2.04447851e-01,  1.20353803e-01, -1.92881361e-01,  1.45889878e-01,\n",
       "       -4.99732010e-02, -1.30403012e-01,  3.63916755e-02,  6.38696700e-02,\n",
       "        5.84882572e-02,  9.17248055e-02, -2.94666439e-02,  3.56133133e-01,\n",
       "       -1.36950940e-01, -5.00562228e-02,  1.68935299e-01,  1.26068398e-01,\n",
       "       -2.29425922e-01, -2.03327066e-03,  2.82410622e-01, -5.22621982e-02,\n",
       "       -1.47676438e-01,  1.48115203e-01, -1.41527846e-01, -6.77790791e-02,\n",
       "        8.55073333e-02,  1.58257589e-01, -9.27265361e-03, -2.52975941e-01,\n",
       "        3.28556485e-02,  7.09870160e-02, -2.14036003e-01,  3.06504369e-01,\n",
       "        2.72237267e-02,  9.82519761e-02, -1.35181457e-01,  9.90812555e-02,\n",
       "       -2.05888137e-01,  1.16345502e-01, -1.60293967e-01, -5.17657474e-02,\n",
       "        1.21842489e-01,  1.48655117e-01, -1.32162366e-02,  1.15314461e-01,\n",
       "       -5.20303287e-02,  1.20715551e-01, -3.73575948e-02, -2.29234938e-02,\n",
       "        1.51485596e-02,  3.36372703e-02, -6.21992946e-02,  7.74607509e-02,\n",
       "        3.11355246e-03, -1.75178364e-01, -1.00695185e-01, -3.81755829e-02,\n",
       "        1.47608042e-01,  6.76660985e-02, -5.32692485e-02, -2.74455249e-02,\n",
       "       -1.20726466e-01, -1.01514146e-01, -7.17153028e-02, -5.16707078e-02,\n",
       "        1.11457832e-01, -1.57852054e-01,  2.44259819e-01,  2.92766150e-02,\n",
       "       -3.68807912e-02,  3.09104789e-02,  1.86270744e-01,  9.65838879e-03,\n",
       "        1.05776684e-02, -7.25914352e-03, -8.87680426e-02,  9.95747186e-03,\n",
       "        2.29622461e-02,  7.43868202e-02, -4.20782762e-03, -1.20795280e-01,\n",
       "       -1.85329050e-01,  2.51578540e-01, -6.39409246e-03,  5.97047582e-02,\n",
       "        2.60821101e-03, -4.99352813e-03, -9.14622471e-02, -1.25717431e-01,\n",
       "       -2.39253312e-01, -3.67688984e-02, -5.08317761e-02, -2.04314776e-02,\n",
       "        3.58134173e-02,  7.94539750e-02,  1.00362569e-01,  7.61166587e-02,\n",
       "        1.32512301e-01,  5.64179756e-02,  1.69763207e-01,  1.30503133e-01,\n",
       "        1.61730014e-02,  1.25962824e-01, -2.15994954e-01, -9.75206196e-02,\n",
       "       -9.85578597e-02,  5.78645729e-02, -1.01189008e-02,  9.10660997e-02,\n",
       "        7.93493465e-02, -1.43866956e-01,  1.90720018e-02,  5.18888012e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#showing vector of individual words\n",
    "wv_model[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/breabeals/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning:\n",
      "\n",
      "Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31420258"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.similarity('love', 'hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Processed_Songs_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
